{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FLIR_groups1and2_train.csv', skiprows=2)\n",
    "df = df.dropna(axis=1, how='all')\n",
    "y_train = df.loc[:, 'aveOralM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking out common features out which are independent of rounds for each subject\n",
    "columns = ['Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance']\n",
    "common_features = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>Female</td>\n",
       "      <td>19</td>\n",
       "      <td>White</td>\n",
       "      <td>24.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>Asian</td>\n",
       "      <td>24.4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>Multiracial</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>White</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender Age                  Ethnicity  T_atm  Humidity  Distance\n",
       "0      Male  46                      White   24.0      28.0      0.80\n",
       "1    Female  36  Black or African-American   24.0      26.0      0.80\n",
       "2    Female  26                      White   24.0      26.0      0.80\n",
       "3    Female  26  Black or African-American   24.0      27.0      0.80\n",
       "4      Male  19                      White   24.0      27.0      0.80\n",
       "..      ...  ..                        ...    ...       ...       ...\n",
       "705  Female  19                      White   24.4      13.5      0.60\n",
       "706  Female  23                      Asian   24.4      14.7      0.63\n",
       "707    Male  23                Multiracial   22.0      30.0      0.60\n",
       "708    Male  19                      White   22.0      30.0      0.60\n",
       "709    Male  19  Black or African-American   22.0      30.0      0.60\n",
       "\n",
       "[710 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_features.loc[:, 'Age'] = [math.ceil((int(x.split('-')[1]) + int(x.split('-')[0]))/2) if '-' in x else int(x.strip('>')) for x in common_features['Age']]\n",
    "common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(min(common_features['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(max(common_features['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "encoded_data = encoder.fit_transform(common_features[['Gender', 'Ethnicity']])\n",
    "\n",
    "# Create a DataFrame from the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Gender', 'Ethnicity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Ethnicity_American Indian or Alaskan Native</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Black or African-American</th>\n",
       "      <th>Ethnicity_Hispanic/Latino</th>\n",
       "      <th>Ethnicity_Multiracial</th>\n",
       "      <th>Ethnicity_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>19</td>\n",
       "      <td>24.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>23</td>\n",
       "      <td>24.4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>23</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>19</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>19</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  T_atm  Humidity  Distance  Gender_Female  Gender_Male  \\\n",
       "0    46   24.0      28.0      0.80            0.0          1.0   \n",
       "1    36   24.0      26.0      0.80            1.0          0.0   \n",
       "2    26   24.0      26.0      0.80            1.0          0.0   \n",
       "3    26   24.0      27.0      0.80            1.0          0.0   \n",
       "4    19   24.0      27.0      0.80            0.0          1.0   \n",
       "..   ..    ...       ...       ...            ...          ...   \n",
       "705  19   24.4      13.5      0.60            1.0          0.0   \n",
       "706  23   24.4      14.7      0.63            1.0          0.0   \n",
       "707  23   22.0      30.0      0.60            0.0          1.0   \n",
       "708  19   22.0      30.0      0.60            0.0          1.0   \n",
       "709  19   22.0      30.0      0.60            0.0          1.0   \n",
       "\n",
       "     Ethnicity_American Indian or Alaskan Native  Ethnicity_Asian  \\\n",
       "0                                            0.0              0.0   \n",
       "1                                            0.0              0.0   \n",
       "2                                            0.0              0.0   \n",
       "3                                            0.0              0.0   \n",
       "4                                            0.0              0.0   \n",
       "..                                           ...              ...   \n",
       "705                                          0.0              0.0   \n",
       "706                                          0.0              1.0   \n",
       "707                                          0.0              0.0   \n",
       "708                                          0.0              0.0   \n",
       "709                                          0.0              0.0   \n",
       "\n",
       "     Ethnicity_Black or African-American  Ethnicity_Hispanic/Latino  \\\n",
       "0                                    0.0                        0.0   \n",
       "1                                    1.0                        0.0   \n",
       "2                                    0.0                        0.0   \n",
       "3                                    1.0                        0.0   \n",
       "4                                    0.0                        0.0   \n",
       "..                                   ...                        ...   \n",
       "705                                  0.0                        0.0   \n",
       "706                                  0.0                        0.0   \n",
       "707                                  0.0                        0.0   \n",
       "708                                  0.0                        0.0   \n",
       "709                                  1.0                        0.0   \n",
       "\n",
       "     Ethnicity_Multiracial  Ethnicity_White  \n",
       "0                      0.0              1.0  \n",
       "1                      0.0              0.0  \n",
       "2                      0.0              1.0  \n",
       "3                      0.0              0.0  \n",
       "4                      0.0              1.0  \n",
       "..                     ...              ...  \n",
       "705                    0.0              1.0  \n",
       "706                    0.0              0.0  \n",
       "707                    1.0              0.0  \n",
       "708                    0.0              1.0  \n",
       "709                    0.0              0.0  \n",
       "\n",
       "[710 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_features = common_features.drop(columns=['Gender', 'Ethnicity'])\n",
    "common_features = pd.concat([common_features, encoded_df], axis=1)\n",
    "\n",
    "common_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "columns_set = {\n",
    "    \n",
    "    'T_offset_' : [1,2,3,4],\n",
    "    'Max1R13_': [1,2,3,4],\n",
    "    'Max1L13_': [1,2,3,4],\n",
    "    'aveAllR13_':  [1,2,3,4],\n",
    "    'aveAllL13_': [1,2,3,4],\n",
    "    'T_RC_' : [1,2,3,4],\n",
    "    'T_RC_Dry_': [1,2,3,4],\n",
    "    'T_RC_Wet_': [1,2,3,4],\n",
    "    'T_RC_Max_': [1,2,3,4],\n",
    "    'T_LC_': [1,2,3,4],\n",
    "    'T_LC_Dry_': [1,2,3,4],\n",
    "    'T_LC_Wet_': [1,2,3,4],\n",
    "    'T_LC_Max_': [1,2,3,4],\n",
    "    'RCC_': [1,2,3,4],\n",
    "    'LCC_': [1,2,3,4],\n",
    "    'canthiMax_': [1,2,3,4],\n",
    "    'canthi4Max_': [1,2,3,4],\n",
    "    'T_FHCC_': [1,2,3,4],\n",
    "    'T_FHRC_': [1,2,3,4],\n",
    "    'T_FHLC_': [1,2,3,4],\n",
    "    'T_FHBC_': [1,2,3,4],\n",
    "    'T_FHTC_': [1,2,3,4],\n",
    "    'T_FH_Max_': [1,2,3,4],\n",
    "    'T_FHC_Max_': [1,2,3,4],\n",
    "    'T_Max_': [1,2,3,4],\n",
    "    'T_OR_': [1,2,3,4],\n",
    "    'T_OR_Max_': [1,2,3,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with NaN values filled with means for thermal data\n",
    "df_filled = pd.DataFrame()\n",
    "column_names = []\n",
    "for header, rounds in columns_set.items():\n",
    "    for roundd in rounds:\n",
    "        column_names.append(f'{header}{roundd}')\n",
    "    \n",
    "thermal_info = df[column_names]\n",
    "thermal_info = thermal_info.fillna(thermal_info.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column wise mean of 4 rounds\n",
    "new_mean_dataframe = pd.DataFrame()\n",
    "\n",
    "for header, rounds in columns_set.items():\n",
    "    column_names = [f'{header}{roundd}' for roundd in rounds]\n",
    "    new_mean_dataframe[f'{header}mean'] = thermal_info[column_names].mean(axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T_offset_mean</th>\n",
       "      <th>Max1R13_mean</th>\n",
       "      <th>Max1L13_mean</th>\n",
       "      <th>aveAllR13_mean</th>\n",
       "      <th>aveAllL13_mean</th>\n",
       "      <th>T_RC_mean</th>\n",
       "      <th>T_RC_Dry_mean</th>\n",
       "      <th>T_RC_Wet_mean</th>\n",
       "      <th>T_RC_Max_mean</th>\n",
       "      <th>T_LC_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Ethnicity_American Indian or Alaskan Native</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Black or African-American</th>\n",
       "      <th>Ethnicity_Hispanic/Latino</th>\n",
       "      <th>Ethnicity_Multiracial</th>\n",
       "      <th>Ethnicity_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7025</td>\n",
       "      <td>35.0300</td>\n",
       "      <td>35.3775</td>\n",
       "      <td>34.4000</td>\n",
       "      <td>34.9175</td>\n",
       "      <td>34.9850</td>\n",
       "      <td>34.9850</td>\n",
       "      <td>34.7625</td>\n",
       "      <td>35.0325</td>\n",
       "      <td>35.3375</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7800</td>\n",
       "      <td>34.5500</td>\n",
       "      <td>34.5200</td>\n",
       "      <td>33.9300</td>\n",
       "      <td>34.2250</td>\n",
       "      <td>34.7100</td>\n",
       "      <td>34.6325</td>\n",
       "      <td>34.6400</td>\n",
       "      <td>34.7425</td>\n",
       "      <td>34.5600</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8625</td>\n",
       "      <td>35.6525</td>\n",
       "      <td>35.5175</td>\n",
       "      <td>34.2775</td>\n",
       "      <td>34.8000</td>\n",
       "      <td>35.6850</td>\n",
       "      <td>35.6675</td>\n",
       "      <td>35.6150</td>\n",
       "      <td>35.7175</td>\n",
       "      <td>35.5025</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9300</td>\n",
       "      <td>35.2225</td>\n",
       "      <td>35.6125</td>\n",
       "      <td>34.3850</td>\n",
       "      <td>35.2475</td>\n",
       "      <td>35.2075</td>\n",
       "      <td>35.2000</td>\n",
       "      <td>35.1175</td>\n",
       "      <td>35.2250</td>\n",
       "      <td>35.5950</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8950</td>\n",
       "      <td>35.5450</td>\n",
       "      <td>35.6650</td>\n",
       "      <td>34.9100</td>\n",
       "      <td>35.3675</td>\n",
       "      <td>35.6025</td>\n",
       "      <td>35.4750</td>\n",
       "      <td>35.5700</td>\n",
       "      <td>35.6400</td>\n",
       "      <td>35.6400</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.9325</td>\n",
       "      <td>35.4800</td>\n",
       "      <td>35.5300</td>\n",
       "      <td>34.9000</td>\n",
       "      <td>34.9900</td>\n",
       "      <td>35.5650</td>\n",
       "      <td>35.5650</td>\n",
       "      <td>35.1350</td>\n",
       "      <td>35.6300</td>\n",
       "      <td>35.5325</td>\n",
       "      <td>...</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.8550</td>\n",
       "      <td>35.6550</td>\n",
       "      <td>35.5325</td>\n",
       "      <td>35.1925</td>\n",
       "      <td>35.2075</td>\n",
       "      <td>35.6125</td>\n",
       "      <td>35.6000</td>\n",
       "      <td>35.4850</td>\n",
       "      <td>35.6550</td>\n",
       "      <td>35.5275</td>\n",
       "      <td>...</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.9700</td>\n",
       "      <td>36.7325</td>\n",
       "      <td>36.4600</td>\n",
       "      <td>36.2225</td>\n",
       "      <td>36.1150</td>\n",
       "      <td>36.7175</td>\n",
       "      <td>36.7150</td>\n",
       "      <td>36.6400</td>\n",
       "      <td>36.7350</td>\n",
       "      <td>36.4350</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>1.0725</td>\n",
       "      <td>36.9450</td>\n",
       "      <td>37.0675</td>\n",
       "      <td>36.3825</td>\n",
       "      <td>36.4825</td>\n",
       "      <td>36.9250</td>\n",
       "      <td>36.9200</td>\n",
       "      <td>36.8200</td>\n",
       "      <td>36.9475</td>\n",
       "      <td>37.0500</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1.0750</td>\n",
       "      <td>36.9825</td>\n",
       "      <td>35.6625</td>\n",
       "      <td>35.8875</td>\n",
       "      <td>35.3825</td>\n",
       "      <td>37.1000</td>\n",
       "      <td>36.9700</td>\n",
       "      <td>37.0350</td>\n",
       "      <td>37.1375</td>\n",
       "      <td>37.0250</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     T_offset_mean  Max1R13_mean  Max1L13_mean  aveAllR13_mean  \\\n",
       "0           0.7025       35.0300       35.3775         34.4000   \n",
       "1           0.7800       34.5500       34.5200         33.9300   \n",
       "2           0.8625       35.6525       35.5175         34.2775   \n",
       "3           0.9300       35.2225       35.6125         34.3850   \n",
       "4           0.8950       35.5450       35.6650         34.9100   \n",
       "..             ...           ...           ...             ...   \n",
       "705         0.9325       35.4800       35.5300         34.9000   \n",
       "706         0.8550       35.6550       35.5325         35.1925   \n",
       "707         0.9700       36.7325       36.4600         36.2225   \n",
       "708         1.0725       36.9450       37.0675         36.3825   \n",
       "709         1.0750       36.9825       35.6625         35.8875   \n",
       "\n",
       "     aveAllL13_mean  T_RC_mean  T_RC_Dry_mean  T_RC_Wet_mean  T_RC_Max_mean  \\\n",
       "0           34.9175    34.9850        34.9850        34.7625        35.0325   \n",
       "1           34.2250    34.7100        34.6325        34.6400        34.7425   \n",
       "2           34.8000    35.6850        35.6675        35.6150        35.7175   \n",
       "3           35.2475    35.2075        35.2000        35.1175        35.2250   \n",
       "4           35.3675    35.6025        35.4750        35.5700        35.6400   \n",
       "..              ...        ...            ...            ...            ...   \n",
       "705         34.9900    35.5650        35.5650        35.1350        35.6300   \n",
       "706         35.2075    35.6125        35.6000        35.4850        35.6550   \n",
       "707         36.1150    36.7175        36.7150        36.6400        36.7350   \n",
       "708         36.4825    36.9250        36.9200        36.8200        36.9475   \n",
       "709         35.3825    37.1000        36.9700        37.0350        37.1375   \n",
       "\n",
       "     T_LC_mean  ...  Humidity  Distance  Gender_Female  Gender_Male  \\\n",
       "0      35.3375  ...      28.0      0.80            0.0          1.0   \n",
       "1      34.5600  ...      26.0      0.80            1.0          0.0   \n",
       "2      35.5025  ...      26.0      0.80            1.0          0.0   \n",
       "3      35.5950  ...      27.0      0.80            1.0          0.0   \n",
       "4      35.6400  ...      27.0      0.80            0.0          1.0   \n",
       "..         ...  ...       ...       ...            ...          ...   \n",
       "705    35.5325  ...      13.5      0.60            1.0          0.0   \n",
       "706    35.5275  ...      14.7      0.63            1.0          0.0   \n",
       "707    36.4350  ...      30.0      0.60            0.0          1.0   \n",
       "708    37.0500  ...      30.0      0.60            0.0          1.0   \n",
       "709    37.0250  ...      30.0      0.60            0.0          1.0   \n",
       "\n",
       "     Ethnicity_American Indian or Alaskan Native  Ethnicity_Asian  \\\n",
       "0                                            0.0              0.0   \n",
       "1                                            0.0              0.0   \n",
       "2                                            0.0              0.0   \n",
       "3                                            0.0              0.0   \n",
       "4                                            0.0              0.0   \n",
       "..                                           ...              ...   \n",
       "705                                          0.0              0.0   \n",
       "706                                          0.0              1.0   \n",
       "707                                          0.0              0.0   \n",
       "708                                          0.0              0.0   \n",
       "709                                          0.0              0.0   \n",
       "\n",
       "     Ethnicity_Black or African-American  Ethnicity_Hispanic/Latino  \\\n",
       "0                                    0.0                        0.0   \n",
       "1                                    1.0                        0.0   \n",
       "2                                    0.0                        0.0   \n",
       "3                                    1.0                        0.0   \n",
       "4                                    0.0                        0.0   \n",
       "..                                   ...                        ...   \n",
       "705                                  0.0                        0.0   \n",
       "706                                  0.0                        0.0   \n",
       "707                                  0.0                        0.0   \n",
       "708                                  0.0                        0.0   \n",
       "709                                  1.0                        0.0   \n",
       "\n",
       "     Ethnicity_Multiracial  Ethnicity_White  \n",
       "0                      0.0              1.0  \n",
       "1                      0.0              0.0  \n",
       "2                      0.0              1.0  \n",
       "3                      0.0              0.0  \n",
       "4                      0.0              1.0  \n",
       "..                     ...              ...  \n",
       "705                    0.0              1.0  \n",
       "706                    0.0              0.0  \n",
       "707                    1.0              0.0  \n",
       "708                    0.0              1.0  \n",
       "709                    0.0              0.0  \n",
       "\n",
       "[710 rows x 39 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.concat([new_mean_dataframe, common_features], axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_original = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_original = y_train.iloc[:].values.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model Creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanTrivialSystem:\n",
    "    def __init__(self):\n",
    "        self_output_value = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.output_value = sum(y) / len(y)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return [self.output_value] * len(X)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1NN\n",
    "\n",
    "class NearestNeighbor1(object):\n",
    "    def __init__(self, p_val: int = 2) -> None:\n",
    "        self.knn_reg = KNeighborsRegressor(\n",
    "            n_neighbors=1,\n",
    "            weights='uniform',\n",
    "            algorithm='auto',\n",
    "            leaf_size=30,\n",
    "            p=p_val,\n",
    "            metric='minkowski',\n",
    "            n_jobs=None\n",
    "        )\n",
    "\n",
    "        self.cv_scores = None\n",
    "\n",
    "    def fit(self, X, y) -> None:\n",
    "        self.knn_reg.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.knn_reg.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.knn_reg.score(X, y)\n",
    "    \n",
    "    def cross_validate(self, X, y, n_splits=5, shuffle=True, random_state=42):\n",
    "        cross_val = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        self.cv_scores = cross_val_score(self.knn_reg, X, y, cv=cross_val)\n",
    "        return self.cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1NN 20-fold cross validation (on normalized dataset)\n",
    "\n",
    "num_splits = 20\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "mae_trains = []\n",
    "mse_trains = []\n",
    "rmse_trains = []\n",
    "\n",
    "mae_vals = []\n",
    "mse_vals = []\n",
    "rmse_vals = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_original):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    NN = NearestNeighbor1()\n",
    "    NN.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    train_pred = NN.predict(X_train_fold)\n",
    "    val_pred = NN.predict(X_val_fold)\n",
    "\n",
    "    # training metrics\n",
    "    mae_train = mean_absolute_error(y_train_fold, train_pred)\n",
    "    mae_trains.append(mae_train)\n",
    "    mse_train = mean_squared_error(y_train_fold, train_pred, squared=True)\n",
    "    mse_trains.append(mse_train)\n",
    "    rmse_train = mean_squared_error(y_train_fold, train_pred, squared=False)\n",
    "    rmse_trains.append(rmse_train)\n",
    "    \n",
    "    # validation metrics\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_pred)\n",
    "    mae_vals.append(mae_val)\n",
    "    mse_val = mean_squared_error(y_val_fold, val_pred, squared=True)\n",
    "    mse_vals.append(mse_val)\n",
    "    rmse_val = mean_squared_error(y_val_fold, val_pred, squared=False)\n",
    "    rmse_vals.append(rmse_val)\n",
    "    \n",
    "    print(f'validation rmse: {rmse_val}')\n",
    "\n",
    "mse_train_mean = sum(mse_trains)/len(mse_trains)\n",
    "rmse_train_mean = sum(rmse_trains)/len(rmse_trains)\n",
    "mae_train_mean = sum(mae_trains)/len(mae_trains) \n",
    "\n",
    "mse_val_mean = sum(mse_vals)/len(mse_vals)\n",
    "rmse_val_mean = sum(rmse_vals)/len(rmse_vals)\n",
    "mae_val_mean = sum(mae_vals)/len(mae_vals)\n",
    "\n",
    "print(f'mae_trains: {mae_trains}')\n",
    "print(f'mean training mae: {mae_train_mean}')\n",
    "print(f'mse_trains: {mse_trains}')\n",
    "print(f'mean training mse: {mse_train_mean}')\n",
    "print(f'rmse_trains: {rmse_trains}')\n",
    "print(f'mean training rmse: {rmse_train_mean}')\n",
    "\n",
    "print(f'mae_vals: {mae_vals}')\n",
    "print(f'mean validation mae: {mae_val_mean}')\n",
    "print(f'mse_vals: {mse_vals}')\n",
    "print(f'mean validation mse: {mse_val_mean}')\n",
    "print(f'rmse_vals: {rmse_vals}')\n",
    "print(f'mean validation rmse: {rmse_val_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2376469760464194\n",
      "0.48723527683216333\n",
      "0.31586001041461065\n",
      "0.23845573174965323\n",
      "0.4839243646729258\n",
      "0.3160925411624781\n"
     ]
    }
   ],
   "source": [
    "# Trivial System\n",
    "\n",
    "train_rmse = []\n",
    "train_mse = []\n",
    "train_mae = []\n",
    "\n",
    "val_rmse = []\n",
    "val_mse = []\n",
    "val_mae = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_original):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_original[train_index], y_train_original[val_index]\n",
    "    \n",
    "\n",
    "    trivial = MeanTrivialSystem()\n",
    "    trivial.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = trivial.predict(X_train_fold)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "    \n",
    "    train_mse.append(mse_train)\n",
    "    train_rmse.append(rmse_train)\n",
    "    train_mae.append(mae_train)\n",
    "    \n",
    "    val_predictions = trivial.predict(X_val_fold)\n",
    "\n",
    "    mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "    \n",
    "    \n",
    "    val_mse.append(mse_val)\n",
    "    val_rmse.append(rmse_val)\n",
    "    val_mae.append(mae_val)\n",
    "\n",
    "print(np.mean(train_mse))       \n",
    "print(np.mean(train_rmse))   \n",
    "print(np.mean(train_mae))   \n",
    "\n",
    "print(np.mean(val_mse))       \n",
    "print(np.mean(val_rmse))   \n",
    "print(np.mean(val_mae))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057964544703018775\n",
      "0.24072384941072983\n",
      "0.18853265961815263\n",
      "0.06709574189571246\n",
      "0.2584805662355625\n",
      "0.20122475247383892\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "train_rmse = []\n",
    "train_mse = []\n",
    "train_mae = []\n",
    "\n",
    "val_rmse = []\n",
    "val_mse = []\n",
    "val_mae = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_original):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_original[train_index], y_train_original[val_index]\n",
    "    \n",
    "\n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    train_predictions = linear_reg.predict(X_train_fold)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "    \n",
    "    train_mse.append(mse_train)\n",
    "    train_rmse.append(rmse_train)\n",
    "    train_mae.append(mae_train)\n",
    "    \n",
    "    val_predictions = linear_reg.predict(X_val_fold)\n",
    "\n",
    "    mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "    \n",
    "    \n",
    "    val_mse.append(mse_val)\n",
    "    val_rmse.append(rmse_val)\n",
    "    val_mae.append(mae_val)\n",
    "\n",
    "print(np.mean(train_mse))       \n",
    "print(np.mean(train_rmse))   \n",
    "print(np.mean(train_mae))   \n",
    "\n",
    "print(np.mean(val_mse))       \n",
    "print(np.mean(val_rmse))   \n",
    "print(np.mean(val_mae))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012565801378451919\n",
      "0.11202671500425414\n",
      "0.09341474212901964\n",
      "0.09405117305786188\n",
      "0.3036622932239355\n",
      "0.22595910142900894\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Regressor\n",
    "\n",
    "def supportvectorreg(X_train_original, y_train_original, kernel='linear', C=10, gamma='auto'):\n",
    "    \n",
    "    train_rmse = []\n",
    "    train_mse = []\n",
    "    train_mae = []\n",
    "\n",
    "    val_rmse = []\n",
    "    val_mse = []\n",
    "    val_mae = []\n",
    "    for train_index, val_index in kf.split(X_train_original):\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_original[train_index], y_train_original[val_index]\n",
    "        \n",
    "        if kernel == 'linear':\n",
    "            clf = svm.SVR(kernel=kernel, C=C)\n",
    "        elif kernel == 'rbf':\n",
    "            clf = svm.SVR(kernel=kernel, C=C, gamma=gamma)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid kernel type.\")\n",
    "\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Make predictions on the training set\n",
    "        train_predictions = clf.predict(X_train_fold)\n",
    "        \n",
    "        mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "        rmse_train = np.sqrt(mse_train)\n",
    "        mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "        \n",
    "        train_mse.append(mse_train)\n",
    "        train_rmse.append(rmse_train)\n",
    "        train_mae.append(mae_train)\n",
    "        \n",
    "        val_predictions = clf.predict(X_val_fold)\n",
    "\n",
    "        mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "        rmse_val = np.sqrt(mse_val)\n",
    "        mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "        \n",
    "        \n",
    "        val_mse.append(mse_val)\n",
    "        val_rmse.append(rmse_val)\n",
    "        val_mae.append(mae_val)\n",
    "        \n",
    "    # Get the number of support vectors\n",
    "    n_support_vectors = np.sum(clf.n_support_)\n",
    "    degrees_of_freedom = len(X_train_original) - n_support_vectors\n",
    "    \n",
    "    print(\"Number of support vectors:\", n_support_vectors)\n",
    "    print(\"Degrees of freedom:\", degrees_of_freedom)\n",
    "    print(np.mean(train_mse))       \n",
    "    print(np.mean(train_rmse))   \n",
    "    print(np.mean(train_mae))   \n",
    "\n",
    "    print(np.mean(val_mse))       \n",
    "    print(np.mean(val_rmse))   \n",
    "    print(np.mean(val_mae))  \n",
    "\n",
    "supportvectorreg(X_train_original, y_train_original, kernel='rbf')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07609151434928177\n",
      "0.27271755420694455\n",
      "0.21473075245227982\n",
      "0.10485347510888317\n",
      "0.31469365673701716\n",
      "0.23388680049074115\n",
      "1.0130947286372543e-26\n",
      "9.914282947283874e-14\n",
      "6.50797100091258e-14\n",
      "1000.8246761809976\n",
      "16.24518176999448\n",
      "2.662029392032099\n",
      "3.62760464945126e-28\n",
      "1.7736302637693773e-14\n",
      "1.0753213655975036e-14\n",
      "1.142467820891807\n",
      "0.9791730062987117\n",
      "0.5167789256304653\n",
      "2.2294266921801808e-26\n",
      "1.3061698507419393e-13\n",
      "6.294057606872128e-14\n",
      "6.76959292841119\n",
      "2.347155976161025\n",
      "0.7717735897141195\n"
     ]
    }
   ],
   "source": [
    "#Polynomial Regression\n",
    "\n",
    "def polynomial_regression(X_train,y_train, orders):\n",
    "  \n",
    "    for order in range(1,orders+1):\n",
    "      \n",
    "        train_rmse = []\n",
    "        train_mse = []\n",
    "        train_mae = []\n",
    "\n",
    "        val_rmse = []\n",
    "        val_mse = []\n",
    "        val_mae = []\n",
    "        \n",
    "        polynomial = PolynomialFeatures(degree=order, include_bias=True)\n",
    "        X_train_polynomial = polynomial.fit_transform(X_train)\n",
    "        print(f'Degree of freedom for order: {order} = {X_train_polynomial.shape[1]}')\n",
    "        \n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "            X_train_fold, X_val_fold = X_train_polynomial[train_index], X_train_polynomial[val_index]\n",
    "            y_train_fold, y_val_fold = y_train_original[train_index], y_train_original[val_index]\n",
    "\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            # Make predictions on the training set\n",
    "            train_predictions = model.predict(X_train_fold)\n",
    "            \n",
    "            mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "            rmse_train = np.sqrt(mse_train)\n",
    "            mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "            \n",
    "            train_mse.append(mse_train)\n",
    "            train_rmse.append(rmse_train)\n",
    "            train_mae.append(mae_train)\n",
    "            \n",
    "            val_predictions = model.predict(X_val_fold)\n",
    "\n",
    "            mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "            rmse_val = np.sqrt(mse_val)\n",
    "            mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "        \n",
    "        \n",
    "            val_mse.append(mse_val)\n",
    "            val_rmse.append(rmse_val)\n",
    "            val_mae.append(mae_val)\n",
    "\n",
    "        print(np.mean(train_mse))       \n",
    "        print(np.mean(train_rmse))   \n",
    "        print(np.mean(train_mae))   \n",
    "\n",
    "        print(np.mean(val_mse))       \n",
    "        print(np.mean(val_rmse))   \n",
    "        print(np.mean(val_mae))  \n",
    "        \n",
    "\n",
    "polynomial_regression(X_train_original, y_train_original, 4)\n",
    "\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00950937966549311\n",
      "0.09747098607868424\n",
      "0.0739843309859161\n",
      "0.06733510809859153\n",
      "0.25880492268764443\n",
      "0.19771338028169\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "def random_forest(X_train,y_train):\n",
    "  \n",
    "      \n",
    "        train_rmse = []\n",
    "        train_mse = []\n",
    "        train_mae = []\n",
    "\n",
    "        val_rmse = []\n",
    "        val_mse = []\n",
    "        val_mae = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            \n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            \n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            train_predictions = model.predict(X_train_fold)\n",
    "            \n",
    "            mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "            rmse_train = np.sqrt(mse_train)\n",
    "            mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "            \n",
    "            train_mse.append(mse_train)\n",
    "            train_rmse.append(rmse_train)\n",
    "            train_mae.append(mae_train)\n",
    "            \n",
    "            val_predictions = model.predict(X_val_fold)\n",
    "\n",
    "            mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "            rmse_val = np.sqrt(mse_val)\n",
    "            mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "        \n",
    "        \n",
    "            val_mse.append(mse_val)\n",
    "            val_rmse.append(rmse_val)\n",
    "            val_mae.append(mae_val)\n",
    "\n",
    "        print(np.mean(train_mse))       \n",
    "        print(np.mean(train_rmse))   \n",
    "        print(np.mean(train_mae))   \n",
    "\n",
    "        print(np.mean(val_mse))       \n",
    "        print(np.mean(val_rmse))   \n",
    "        print(np.mean(val_mae)) \n",
    "        \n",
    "\n",
    "random_forest(X_train_original, y_train_original)\n",
    "\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF Neural Network (RBF Sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_feature_linearreg = RBFSampler(gamma=1, random_state=1)\n",
    "regressor_linearreg = LinearRegression()\n",
    "\n",
    "rbf_model_linearreg_sampler = make_pipeline(rbf_feature_linearreg, regressor_linearreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_feature_svc = RBFSampler(gamma=1, random_state=1)\n",
    "regressor_svc = svm.SVR(kernel='linear', C=1.0, gamma='auto')\n",
    "\n",
    "rbf_model_svc_rbf_sampler = make_pipeline(rbf_feature_svc, regressor_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF Neural Network 20-fold cross validation\n",
    "# --> linear reg with RBF Sampler (normalized)\n",
    "\n",
    "num_splits = 20\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "mae_trains = []\n",
    "mae_vals = []\n",
    "mse_trains = []\n",
    "mse_vals = []\n",
    "rmse_trains = []\n",
    "rmse_vals = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "     \n",
    "    rbf_model_linearreg_sampler.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # training set predictions\n",
    "    train_pred = rbf_model_linearreg_sampler.predict(X_train_fold)\n",
    "    \n",
    "    mae_train = mean_absolute_error(y_train_fold, train_pred)\n",
    "    mae_trains.append(mae_train)\n",
    "    mse_train = mean_squared_error(y_train_fold, train_pred, squared=True)\n",
    "    mse_trains.append(mse_train)\n",
    "    rmse_train = mean_squared_error(y_train_fold, train_pred, squared=False)\n",
    "    rmse_trains.append(rmse_train)\n",
    "    # mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "    \n",
    "    print(f'training rmse: {rmse_train}')\n",
    "    # print(f'training mse: {mse_train}')\n",
    "    \n",
    "    # predictions on the validation set\n",
    "    val_pred = rbf_model_linearreg_sampler.predict(X_val_fold)\n",
    "\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_pred)\n",
    "    mae_vals.append(mae_val)\n",
    "    mse_val = mean_squared_error(y_val_fold, val_pred, squared=True)\n",
    "    mse_vals.append(mse_val)\n",
    "    rmse_val = mean_squared_error(y_val_fold, val_pred, squared=False)\n",
    "    rmse_vals.append(rmse_val)\n",
    "    # mse_val = mean_squared_error(y_val_fold, val_pred)\n",
    "    \n",
    "    print(f'validation rmse: {rmse_val}')\n",
    "    # print(f'validation mse: {mse_val}')\n",
    "\n",
    "mae_train_mean = sum(mae_trains)/len(mae_trains)\n",
    "mse_train_mean = sum(mse_trains)/len(mse_trains)\n",
    "rmse_train_mean = sum(rmse_trains)/len(rmse_trains)\n",
    "\n",
    "mae_val_mean = sum(mae_vals)/len(mae_vals)\n",
    "mse_val_mean = sum(mse_vals)/len(mse_vals)\n",
    "rmse_val_mean = sum(rmse_vals)/len(rmse_vals)\n",
    "\n",
    "print(f'mean training mae: {mae_train_mean}')\n",
    "print(f'mean training mse: {mse_train_mean}')\n",
    "print(f'mean training rmse: {rmse_train_mean}')\n",
    "print(f'mean validation mae: {mae_val_mean}')\n",
    "print(f'mean validation mse: {mse_val_mean}')\n",
    "print(f'mean validation rmse: {rmse_val_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF Neural Network 20-fold cross validation\n",
    "# --> svc with RBF Sampler (normalized)\n",
    "\n",
    "num_splits = 20\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "mae_trains = []\n",
    "mae_vals = []\n",
    "mse_trains = []\n",
    "mse_vals = []\n",
    "rmse_trains = []\n",
    "rmse_vals = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "     \n",
    "    rbf_model_svc_rbf_sampler.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # training set predictions\n",
    "    train_pred = rbf_model_svc_rbf_sampler.predict(X_train_fold)\n",
    "    \n",
    "    mae_train = mean_absolute_error(y_train_fold, train_pred)\n",
    "    mae_trains.append(mae_train)\n",
    "    mse_train = mean_squared_error(y_train_fold, train_pred, squared=True)\n",
    "    mse_trains.append(mse_train)\n",
    "    rmse_train = mean_squared_error(y_train_fold, train_pred, squared=False)\n",
    "    rmse_trains.append(rmse_train)\n",
    "    # mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "    \n",
    "    print(f'training rmse: {rmse_train}')\n",
    "    # print(f'training mse: {mse_train}')\n",
    "    \n",
    "    # predictions on the validation set\n",
    "    val_pred = rbf_model_svc_rbf_sampler.predict(X_val_fold)\n",
    "\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_pred)\n",
    "    mae_vals.append(mae_val)\n",
    "    mse_val = mean_squared_error(y_val_fold, val_pred, squared=True)\n",
    "    mse_vals.append(mse_val)\n",
    "    rmse_val = mean_squared_error(y_val_fold, val_pred, squared=False)\n",
    "    rmse_vals.append(rmse_val)\n",
    "    # mse_val = mean_squared_error(y_val_fold, val_pred)\n",
    "    \n",
    "    print(f'validation rmse: {rmse_val}')\n",
    "    # print(f'validation mse: {mse_val}')\n",
    "\n",
    "mae_train_mean = sum(mae_trains)/len(mae_trains)\n",
    "mse_train_mean = sum(mse_trains)/len(mse_trains)\n",
    "rmse_train_mean = sum(rmse_trains)/len(rmse_trains)\n",
    "\n",
    "mae_val_mean = sum(mae_vals)/len(mae_vals)\n",
    "mse_val_mean = sum(mse_vals)/len(mse_vals)\n",
    "rmse_val_mean = sum(rmse_vals)/len(rmse_vals)\n",
    "\n",
    "print(f'mean training mae: {mae_train_mean}')\n",
    "print(f'mean training mse: {mse_train_mean}')\n",
    "print(f'mean training rmse: {rmse_train_mean}')\n",
    "print(f'mean validation mae: {mae_val_mean}')\n",
    "print(f'mean validation mse: {mse_val_mean}')\n",
    "print(f'mean validation rmse: {rmse_val_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF Network (K Means Clustering for RBF Centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centers = 10 \n",
    "kmeans = KMeans(n_clusters=n_centers, random_state=0)\n",
    "kmeans.fit(X_train_original) # change for normalized part\n",
    "centers = kmeans.cluster_centers_\n",
    "gamma = 1.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rbf = rbf_kernel(X_train, centers, gamma=gamma)\n",
    "print(X_rbf)\n",
    "\n",
    "X_rbf = rbf_kernel(X_train_original, centers, gamma=gamma)\n",
    "print(X_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "kf = KFold(n_splits=20, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation (unnormalized)\n",
    "mae_trains = []\n",
    "mse_trains = []\n",
    "rmse_trains = []\n",
    "\n",
    "mae_vals = []\n",
    "mse_vals = []\n",
    "rmse_vals = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_rbf):\n",
    "    X_train_fold, X_val_fold = X_rbf[train_index], X_rbf[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    train_pred = model.predict(X_train_fold)\n",
    "    val_pred = model.predict(X_val_fold)\n",
    "\n",
    "    mae_train = mean_absolute_error(y_train_fold, train_pred)\n",
    "    mae_trains.append(mae_train)\n",
    "    mse_train = mean_squared_error(y_train_fold, train_pred, squared=True)\n",
    "    mse_trains.append(mse_train)\n",
    "    rmse_train = mean_squared_error(y_train_fold, train_pred, squared=False)\n",
    "    rmse_trains.append(rmse_train)\n",
    "\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_pred)\n",
    "    mae_vals.append(mae_val)\n",
    "    mse_val = mean_squared_error(y_val_fold, val_pred, squared=True)\n",
    "    mse_vals.append(mse_val)\n",
    "    rmse_val = mean_squared_error(y_val_fold, val_pred, squared=False)\n",
    "    rmse_vals.append(rmse_val)\n",
    "\n",
    "    print(f'Training RMSE: {rmse_train}, Validation RMSE: {rmse_val}')\n",
    "    \n",
    "mean_mae_train = sum(mae_trains) / len(mae_trains)\n",
    "mean_mse_train = sum(mse_trains) / len(mse_trains)\n",
    "mean_rmse_train = sum(rmse_trains) / len(rmse_trains)\n",
    "mean_mae_val = sum(mae_vals) / len(mae_vals)\n",
    "mean_mse_val = sum(mse_vals) / len(mse_vals)\n",
    "mean_rmse_val = sum(rmse_vals) / len(rmse_vals)\n",
    "    \n",
    "# average RMSE\n",
    "print(f'Average Training MAE: {mean_mae_train}')\n",
    "print(f'Average Training MSE: {mean_mse_train}')\n",
    "print(f'Average Training RMSE: {mean_rmse_train}')\n",
    "print(f'Average Validation MAE: {mean_mae_val}')\n",
    "print(f'Average Validation MSE: {mean_mse_val}')\n",
    "print(f'Average Validation RMSE: {mean_rmse_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation (normalized)\n",
    "mae_trains = []\n",
    "mse_trains = []\n",
    "rmse_trains = []\n",
    "\n",
    "mae_vals = []\n",
    "mse_vals = []\n",
    "rmse_vals = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_rbf):\n",
    "    X_train_fold, X_val_fold = X_rbf[train_index], X_rbf[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    train_pred = model.predict(X_train_fold)\n",
    "    val_pred = model.predict(X_val_fold)\n",
    "\n",
    "    mae_train = mean_absolute_error(y_train_fold, train_pred)\n",
    "    mae_trains.append(mae_train)\n",
    "    mse_train = mean_squared_error(y_train_fold, train_pred, squared=True)\n",
    "    mse_trains.append(mse_train)\n",
    "    rmse_train = mean_squared_error(y_train_fold, train_pred, squared=False)\n",
    "    rmse_trains.append(rmse_train)\n",
    "\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_pred)\n",
    "    mae_vals.append(mae_val)\n",
    "    mse_val = mean_squared_error(y_val_fold, val_pred, squared=True)\n",
    "    mse_vals.append(mse_val)\n",
    "    rmse_val = mean_squared_error(y_val_fold, val_pred, squared=False)\n",
    "    rmse_vals.append(rmse_val)\n",
    "\n",
    "    print(f'Training RMSE: {rmse_train}, Validation RMSE: {rmse_val}')\n",
    "    \n",
    "mean_mae_train = sum(mae_trains) / len(mae_trains)\n",
    "mean_mse_train = sum(mse_trains) / len(mse_trains)\n",
    "mean_rmse_train = sum(rmse_trains) / len(rmse_trains)\n",
    "mean_mae_val = sum(mae_vals) / len(mae_vals)\n",
    "mean_mse_val = sum(mse_vals) / len(mse_vals)\n",
    "mean_rmse_val = sum(rmse_vals) / len(rmse_vals)\n",
    "    \n",
    "# average RMSE\n",
    "print(f'Average Training MAE: {mean_mae_train}')\n",
    "print(f'Average Training MSE: {mean_mse_train}')\n",
    "print(f'Average Training RMSE: {mean_rmse_train}')\n",
    "print(f'Average Validation MAE: {mean_mae_val}')\n",
    "print(f'Average Validation MSE: {mean_mse_val}')\n",
    "print(f'Average Validation RMSE: {mean_rmse_val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF Network (ANN Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for normalized dataset\n",
    "X_train_np = X_train_original \n",
    "y_train_np = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFNetwork(nn.Module):\n",
    "    def __init__(self, centers, num_outputs):\n",
    "        super(RBFNetwork, self).__init__()\n",
    "        self.num_centers = centers.shape[0]\n",
    "        self.centers = nn.Parameter(torch.tensor(centers, dtype=torch.float32), requires_grad=True)\n",
    "        self.spreads = nn.Parameter(torch.ones(self.num_centers), requires_grad=True)\n",
    "        self.linear = nn.Linear(self.num_centers, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) - self.centers.unsqueeze(0)\n",
    "        rbf_activations = torch.exp(-self.spreads.unsqueeze(0).unsqueeze(0) * (x ** 2).sum(2))\n",
    "        out = self.linear(rbf_activations)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10 fold RBF Network ANN Implementation\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# RBF network parameters\n",
    "num_outputs = 1\n",
    "centers_initial = kmeans.cluster_centers_ \n",
    "\n",
    "# 5 fold mean rmse\n",
    "fold_rmse_val = []\n",
    "fold_rmse_train = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_tensor)):\n",
    "    print(f'Fold {fold + 1}/{num_folds}')\n",
    "    \n",
    "    X_train, y_train = X_train_tensor[train_idx], y_train_tensor[train_idx]\n",
    "    X_val, y_val = X_train_tensor[val_idx], y_train_tensor[val_idx]\n",
    "\n",
    "    model = RBFNetwork(centers=centers_initial, num_outputs=num_outputs)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # training loop\n",
    "    num_epochs = 10000 \n",
    "    for epoch in range(num_epochs):\n",
    "        # model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.forward(X_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print loss every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    model.eval()  #  evaluation mode \n",
    "    with torch.no_grad():\n",
    "        train_outputs_post = model(X_train)\n",
    "        train_loss_post = criterion(train_outputs_post, y_train)\n",
    "\n",
    "    # validation\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "    \n",
    "    print(f'MSE Validation Loss for Fold {fold + 1}: {val_loss.item()}')\n",
    "    print(f'RMSE Validation Loss for Fold {fold + 1}: {math.sqrt(val_loss.item())}')\n",
    "    fold_rmse_val.append(math.sqrt(val_loss.item()))\n",
    "    fold_rmse_train.append(math.sqrt(train_loss_post.item()))\n",
    "\n",
    "    # reset the centers (re-initialized each fold)\n",
    "    model.centers = nn.Parameter(torch.tensor(kmeans.cluster_centers_, dtype=torch.float32), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_rmse_mean_val = sum(fold_rmse_val) / len(fold_rmse_val)\n",
    "fold_rmse_mean_train = sum(fold_rmse_train) / len(fold_rmse_train)\n",
    "print(f'10 fold mean rmse val: {fold_rmse_mean_val}')\n",
    "print(f'10 fold mean rmse train: {fold_rmse_mean_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On an earlier iteration: \n",
    "Best Results:-\n",
    "- lr = 0.001, epochs = 10000\n",
    "- [0.25580558502633516, 0.4197435754564935, 0.4785440864662346, 0.3611876343480811, 0.44053788981142666, 0.25008861640104796, 0.5463839097074379, 0.426668659735783, 0.19947390018180974, 0.2637490336233216]\n",
    "- 10 fold mean rmse: 0.3642182890757971"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FLIR_groups1and2_test.csv', skiprows=2)\n",
    "df = df.dropna(axis=1, how='all')\n",
    "X_test = df.iloc[:, :-1]\n",
    "y_test = df.loc[:, 'aveOralM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking out common features out which are independent of rounds for each subject\n",
    "columns = ['Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance']\n",
    "common_features = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>19</td>\n",
       "      <td>Asian</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>19</td>\n",
       "      <td>White</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>Asian</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>Asian</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>White</td>\n",
       "      <td>24.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>Asian</td>\n",
       "      <td>25.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>White</td>\n",
       "      <td>25.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Female</td>\n",
       "      <td>19</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>Hispanic/Latino</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Female</td>\n",
       "      <td>19</td>\n",
       "      <td>White</td>\n",
       "      <td>23.8</td>\n",
       "      <td>45.6</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender Age                  Ethnicity  T_atm  Humidity  Distance\n",
       "0    Female  19                      Asian   22.0      30.0      0.60\n",
       "1    Female  19                      White   24.1      15.6      0.62\n",
       "2      Male  19                      Asian   24.1      15.6      0.62\n",
       "3      Male  23                      Asian   24.1      15.6      0.66\n",
       "4      Male  46                      White   24.1      18.0      0.60\n",
       "..      ...  ..                        ...    ...       ...       ...\n",
       "305  Female  23                      Asian   25.7      50.8      0.60\n",
       "306  Female  23                      White   25.7      50.8      0.60\n",
       "307  Female  19  Black or African-American   28.0      24.3      0.60\n",
       "308    Male  28            Hispanic/Latino   25.0      39.8      0.60\n",
       "309  Female  19                      White   23.8      45.6      0.60\n",
       "\n",
       "[310 rows x 6 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_features.loc[:, 'Age'] = [math.ceil((int(x.split('-')[1]) + int(x.split('-')[0]))/2) if '-' in x else int(x.strip('>')) for x in common_features['Age']]\n",
    "common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "encoded_data = encoder.fit_transform(common_features[['Gender', 'Ethnicity']])\n",
    "\n",
    "# Create a DataFrame from the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Gender', 'Ethnicity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Black or African-American</th>\n",
       "      <th>Ethnicity_Hispanic/Latino</th>\n",
       "      <th>Ethnicity_Multiracial</th>\n",
       "      <th>Ethnicity_White</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Ethnicity_American Indian or Alaskan Native</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.8</td>\n",
       "      <td>45.6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender_Female  Gender_Male  Ethnicity_Asian  \\\n",
       "0    19            1.0          0.0              1.0   \n",
       "1    19            1.0          0.0              0.0   \n",
       "2    19            0.0          1.0              1.0   \n",
       "3    23            0.0          1.0              1.0   \n",
       "4    46            0.0          1.0              0.0   \n",
       "..   ..            ...          ...              ...   \n",
       "305  23            1.0          0.0              1.0   \n",
       "306  23            1.0          0.0              0.0   \n",
       "307  19            1.0          0.0              0.0   \n",
       "308  28            0.0          1.0              0.0   \n",
       "309  19            1.0          0.0              0.0   \n",
       "\n",
       "     Ethnicity_Black or African-American  Ethnicity_Hispanic/Latino  \\\n",
       "0                                    0.0                        0.0   \n",
       "1                                    0.0                        0.0   \n",
       "2                                    0.0                        0.0   \n",
       "3                                    0.0                        0.0   \n",
       "4                                    0.0                        0.0   \n",
       "..                                   ...                        ...   \n",
       "305                                  0.0                        0.0   \n",
       "306                                  0.0                        0.0   \n",
       "307                                  1.0                        0.0   \n",
       "308                                  0.0                        1.0   \n",
       "309                                  0.0                        0.0   \n",
       "\n",
       "     Ethnicity_Multiracial  Ethnicity_White  T_atm  Humidity  Distance  \\\n",
       "0                      0.0              0.0   22.0      30.0      0.60   \n",
       "1                      0.0              1.0   24.1      15.6      0.62   \n",
       "2                      0.0              0.0   24.1      15.6      0.62   \n",
       "3                      0.0              0.0   24.1      15.6      0.66   \n",
       "4                      0.0              1.0   24.1      18.0      0.60   \n",
       "..                     ...              ...    ...       ...       ...   \n",
       "305                    0.0              0.0   25.7      50.8      0.60   \n",
       "306                    0.0              1.0   25.7      50.8      0.60   \n",
       "307                    0.0              0.0   28.0      24.3      0.60   \n",
       "308                    0.0              0.0   25.0      39.8      0.60   \n",
       "309                    0.0              1.0   23.8      45.6      0.60   \n",
       "\n",
       "     Ethnicity_American Indian or Alaskan Native  \n",
       "0                                            0.0  \n",
       "1                                            0.0  \n",
       "2                                            0.0  \n",
       "3                                            0.0  \n",
       "4                                            0.0  \n",
       "..                                           ...  \n",
       "305                                          0.0  \n",
       "306                                          0.0  \n",
       "307                                          0.0  \n",
       "308                                          0.0  \n",
       "309                                          0.0  \n",
       "\n",
       "[310 rows x 12 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_features = common_features.drop(columns=['Gender', 'Ethnicity'])\n",
    "common_features = pd.concat([common_features, encoded_df], axis=1)\n",
    "\n",
    "remaning_columns = ['T_atm', 'Humidity', 'Distance'] \n",
    "remaning_features = X_test[remaning_columns].fillna(X_test[remaning_columns].mean())\n",
    "common_features = common_features.drop(columns=remaning_columns)\n",
    "common_features = pd.concat([common_features, remaning_features], axis=1)\n",
    "\n",
    "common_features['Ethnicity_American Indian or Alaskan Native'] = 0.0\n",
    "common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "columns_set = {\n",
    "    \n",
    "    'T_offset_' : [1,2,3,4],\n",
    "    'Max1R13_': [1,2,3,4],\n",
    "    'Max1L13_': [1,2,3,4],\n",
    "    'aveAllR13_':  [1,2,3,4],\n",
    "    'aveAllL13_': [1,2,3,4],\n",
    "    'T_RC_' : [1,2,3,4],\n",
    "    'T_RC_Dry_': [1,2,3,4],\n",
    "    'T_RC_Wet_': [1,2,3,4],\n",
    "    'T_RC_Max_': [1,2,3,4],\n",
    "    'T_LC_': [1,2,3,4],\n",
    "    'T_LC_Dry_': [1,2,3,4],\n",
    "    'T_LC_Wet_': [1,2,3,4],\n",
    "    'T_LC_Max_': [1,2,3,4],\n",
    "    'RCC_': [1,2,3,4],\n",
    "    'LCC_': [1,2,3,4],\n",
    "    'canthiMax_': [1,2,3,4],\n",
    "    'canthi4Max_': [1,2,3,4],\n",
    "    'T_FHCC_': [1,2,3,4],\n",
    "    'T_FHRC_': [1,2,3,4],\n",
    "    'T_FHLC_': [1,2,3,4],\n",
    "    'T_FHBC_': [1,2,3,4],\n",
    "    'T_FHTC_': [1,2,3,4],\n",
    "    'T_FH_Max_': [1,2,3,4],\n",
    "    'T_FHC_Max_': [1,2,3,4],\n",
    "    'T_Max_': [1,2,3,4],\n",
    "    'T_OR_': [1,2,3,4],\n",
    "    'T_OR_Max_': [1,2,3,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with NaN values filled with means for thermal data\n",
    "df_filled = pd.DataFrame()\n",
    "column_names = []\n",
    "for header, rounds in columns_set.items():\n",
    "    for roundd in rounds:\n",
    "        column_names.append(f'{header}{roundd}')\n",
    "    \n",
    "thermal_info = df[column_names]\n",
    "thermal_info = thermal_info.fillna(thermal_info.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column wise mean of 4 rounds\n",
    "new_mean_dataframe = pd.DataFrame()\n",
    "\n",
    "for header, rounds in columns_set.items():\n",
    "    column_names = [f'{header}{roundd}' for roundd in rounds]\n",
    "    new_mean_dataframe[f'{header}mean'] = thermal_info[column_names].mean(axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([new_mean_dataframe, common_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T_offset_mean</th>\n",
       "      <th>Max1R13_mean</th>\n",
       "      <th>Max1L13_mean</th>\n",
       "      <th>aveAllR13_mean</th>\n",
       "      <th>aveAllL13_mean</th>\n",
       "      <th>T_RC_mean</th>\n",
       "      <th>T_RC_Dry_mean</th>\n",
       "      <th>T_RC_Wet_mean</th>\n",
       "      <th>T_RC_Max_mean</th>\n",
       "      <th>T_LC_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Black or African-American</th>\n",
       "      <th>Ethnicity_Hispanic/Latino</th>\n",
       "      <th>Ethnicity_Multiracial</th>\n",
       "      <th>Ethnicity_White</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Ethnicity_American Indian or Alaskan Native</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.067500</td>\n",
       "      <td>35.600000</td>\n",
       "      <td>35.332500</td>\n",
       "      <td>35.305000</td>\n",
       "      <td>35.002500</td>\n",
       "      <td>35.585000</td>\n",
       "      <td>35.585000</td>\n",
       "      <td>35.395000</td>\n",
       "      <td>35.600000</td>\n",
       "      <td>35.390000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555000</td>\n",
       "      <td>36.112500</td>\n",
       "      <td>36.237500</td>\n",
       "      <td>35.535000</td>\n",
       "      <td>35.810000</td>\n",
       "      <td>36.187500</td>\n",
       "      <td>36.187500</td>\n",
       "      <td>35.637500</td>\n",
       "      <td>36.200000</td>\n",
       "      <td>36.205000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.767500</td>\n",
       "      <td>37.620000</td>\n",
       "      <td>37.130000</td>\n",
       "      <td>37.162500</td>\n",
       "      <td>36.530000</td>\n",
       "      <td>37.695000</td>\n",
       "      <td>37.567500</td>\n",
       "      <td>37.690000</td>\n",
       "      <td>37.740000</td>\n",
       "      <td>37.140000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>35.490000</td>\n",
       "      <td>35.627500</td>\n",
       "      <td>34.865000</td>\n",
       "      <td>34.962500</td>\n",
       "      <td>35.490000</td>\n",
       "      <td>35.482500</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>35.517500</td>\n",
       "      <td>35.622500</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.877500</td>\n",
       "      <td>34.735000</td>\n",
       "      <td>34.660000</td>\n",
       "      <td>34.102500</td>\n",
       "      <td>34.185000</td>\n",
       "      <td>34.747500</td>\n",
       "      <td>34.747500</td>\n",
       "      <td>34.377500</td>\n",
       "      <td>34.790000</td>\n",
       "      <td>34.672500</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1.222500</td>\n",
       "      <td>35.642500</td>\n",
       "      <td>35.652500</td>\n",
       "      <td>34.857500</td>\n",
       "      <td>35.005000</td>\n",
       "      <td>35.752500</td>\n",
       "      <td>35.657500</td>\n",
       "      <td>35.737500</td>\n",
       "      <td>35.775000</td>\n",
       "      <td>35.802500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1.467500</td>\n",
       "      <td>35.982500</td>\n",
       "      <td>35.757500</td>\n",
       "      <td>35.427500</td>\n",
       "      <td>35.197500</td>\n",
       "      <td>35.970000</td>\n",
       "      <td>35.950000</td>\n",
       "      <td>35.862500</td>\n",
       "      <td>36.007500</td>\n",
       "      <td>35.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.130000</td>\n",
       "      <td>36.407500</td>\n",
       "      <td>36.340000</td>\n",
       "      <td>35.870000</td>\n",
       "      <td>35.960000</td>\n",
       "      <td>36.410000</td>\n",
       "      <td>36.362500</td>\n",
       "      <td>36.365000</td>\n",
       "      <td>36.447500</td>\n",
       "      <td>36.302500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1.138141</td>\n",
       "      <td>35.764716</td>\n",
       "      <td>35.639493</td>\n",
       "      <td>34.641506</td>\n",
       "      <td>34.575435</td>\n",
       "      <td>35.783421</td>\n",
       "      <td>35.697344</td>\n",
       "      <td>35.732854</td>\n",
       "      <td>35.820831</td>\n",
       "      <td>35.792977</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.867500</td>\n",
       "      <td>35.707500</td>\n",
       "      <td>35.582500</td>\n",
       "      <td>34.887500</td>\n",
       "      <td>35.087500</td>\n",
       "      <td>35.702500</td>\n",
       "      <td>35.680000</td>\n",
       "      <td>35.687500</td>\n",
       "      <td>35.725000</td>\n",
       "      <td>35.597500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.8</td>\n",
       "      <td>45.6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     T_offset_mean  Max1R13_mean  Max1L13_mean  aveAllR13_mean  \\\n",
       "0         1.067500     35.600000     35.332500       35.305000   \n",
       "1         0.555000     36.112500     36.237500       35.535000   \n",
       "2         0.767500     37.620000     37.130000       37.162500   \n",
       "3         0.850000     35.490000     35.627500       34.865000   \n",
       "4         0.877500     34.735000     34.660000       34.102500   \n",
       "..             ...           ...           ...             ...   \n",
       "305       1.222500     35.642500     35.652500       34.857500   \n",
       "306       1.467500     35.982500     35.757500       35.427500   \n",
       "307       0.130000     36.407500     36.340000       35.870000   \n",
       "308       1.138141     35.764716     35.639493       34.641506   \n",
       "309       0.867500     35.707500     35.582500       34.887500   \n",
       "\n",
       "     aveAllL13_mean  T_RC_mean  T_RC_Dry_mean  T_RC_Wet_mean  T_RC_Max_mean  \\\n",
       "0         35.002500  35.585000      35.585000      35.395000      35.600000   \n",
       "1         35.810000  36.187500      36.187500      35.637500      36.200000   \n",
       "2         36.530000  37.695000      37.567500      37.690000      37.740000   \n",
       "3         34.962500  35.490000      35.482500      35.300000      35.517500   \n",
       "4         34.185000  34.747500      34.747500      34.377500      34.790000   \n",
       "..              ...        ...            ...            ...            ...   \n",
       "305       35.005000  35.752500      35.657500      35.737500      35.775000   \n",
       "306       35.197500  35.970000      35.950000      35.862500      36.007500   \n",
       "307       35.960000  36.410000      36.362500      36.365000      36.447500   \n",
       "308       34.575435  35.783421      35.697344      35.732854      35.820831   \n",
       "309       35.087500  35.702500      35.680000      35.687500      35.725000   \n",
       "\n",
       "     T_LC_mean  ...  Gender_Male  Ethnicity_Asian  \\\n",
       "0    35.390000  ...          0.0              1.0   \n",
       "1    36.205000  ...          0.0              0.0   \n",
       "2    37.140000  ...          1.0              1.0   \n",
       "3    35.622500  ...          1.0              1.0   \n",
       "4    34.672500  ...          1.0              0.0   \n",
       "..         ...  ...          ...              ...   \n",
       "305  35.802500  ...          0.0              1.0   \n",
       "306  35.825000  ...          0.0              0.0   \n",
       "307  36.302500  ...          0.0              0.0   \n",
       "308  35.792977  ...          1.0              0.0   \n",
       "309  35.597500  ...          0.0              0.0   \n",
       "\n",
       "     Ethnicity_Black or African-American  Ethnicity_Hispanic/Latino  \\\n",
       "0                                    0.0                        0.0   \n",
       "1                                    0.0                        0.0   \n",
       "2                                    0.0                        0.0   \n",
       "3                                    0.0                        0.0   \n",
       "4                                    0.0                        0.0   \n",
       "..                                   ...                        ...   \n",
       "305                                  0.0                        0.0   \n",
       "306                                  0.0                        0.0   \n",
       "307                                  1.0                        0.0   \n",
       "308                                  0.0                        1.0   \n",
       "309                                  0.0                        0.0   \n",
       "\n",
       "     Ethnicity_Multiracial  Ethnicity_White  T_atm  Humidity  Distance  \\\n",
       "0                      0.0              0.0   22.0      30.0      0.60   \n",
       "1                      0.0              1.0   24.1      15.6      0.62   \n",
       "2                      0.0              0.0   24.1      15.6      0.62   \n",
       "3                      0.0              0.0   24.1      15.6      0.66   \n",
       "4                      0.0              1.0   24.1      18.0      0.60   \n",
       "..                     ...              ...    ...       ...       ...   \n",
       "305                    0.0              0.0   25.7      50.8      0.60   \n",
       "306                    0.0              1.0   25.7      50.8      0.60   \n",
       "307                    0.0              0.0   28.0      24.3      0.60   \n",
       "308                    0.0              0.0   25.0      39.8      0.60   \n",
       "309                    0.0              1.0   23.8      45.6      0.60   \n",
       "\n",
       "     Ethnicity_American Indian or Alaskan Native  \n",
       "0                                            0.0  \n",
       "1                                            0.0  \n",
       "2                                            0.0  \n",
       "3                                            0.0  \n",
       "4                                            0.0  \n",
       "..                                           ...  \n",
       "305                                          0.0  \n",
       "306                                          0.0  \n",
       "307                                          0.0  \n",
       "308                                          0.0  \n",
       "309                                          0.0  \n",
       "\n",
       "[310 rows x 39 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_test_original = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_original = y_test.iloc[:].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013305234793008313\n",
      "0.11534831941995649\n",
      "0.09480329696073524\n",
      "0.22329070085905017\n",
      "0.4725364545292248\n",
      "0.32053458457244977\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(X_train_original, y_train_original, X_test_original, y_test_original, kernel='linear', C=10, gamma='auto'):\n",
    "    \n",
    "    if kernel == 'linear':\n",
    "            clf = svm.SVR(kernel=kernel, C=C)\n",
    "    elif kernel == 'rbf':\n",
    "            clf = svm.SVR(kernel=kernel, C=C, gamma=gamma)\n",
    "    else:\n",
    "            raise ValueError(\"Invalid kernel type.\")\n",
    "\n",
    "    clf.fit(X_train_original, y_train_original)\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = clf.predict(X_train_original)\n",
    "        \n",
    "    train_mse = mean_squared_error(y_train_original, train_predictions)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_mae = mean_absolute_error(y_train_original, train_predictions)\n",
    "    \n",
    "    test_predictions = clf.predict(X_test_original)\n",
    "\n",
    "    test_mse = mean_squared_error(y_test_original, test_predictions)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(y_test_original, test_predictions)\n",
    "        \n",
    "    print(train_mse)       \n",
    "    print(train_rmse)   \n",
    "    print(train_mae)\n",
    "    \n",
    "    print(test_mse)       \n",
    "    print(test_rmse)   \n",
    "    print(test_mae) \n",
    "\n",
    "train_and_evaluate(X_train_original, y_train_original, X_test_original, y_test_original, kernel='rbf')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee541_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FLIR_groups1and2_train.csv', skiprows=2)\n",
    "df = df.dropna(axis=1, how='all')\n",
    "y_train = df.loc[:, 'aveOralM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking out common features out which are independent of rounds for each subject\n",
    "columns = ['Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Distance']\n",
    "common_features = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>Female</td>\n",
       "      <td>19</td>\n",
       "      <td>White</td>\n",
       "      <td>24.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>Female</td>\n",
       "      <td>23</td>\n",
       "      <td>Asian</td>\n",
       "      <td>24.4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>Male</td>\n",
       "      <td>23</td>\n",
       "      <td>Multiracial</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>White</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender Age                  Ethnicity  T_atm  Humidity  Distance\n",
       "0      Male  46                      White   24.0      28.0      0.80\n",
       "1    Female  36  Black or African-American   24.0      26.0      0.80\n",
       "2    Female  26                      White   24.0      26.0      0.80\n",
       "3    Female  26  Black or African-American   24.0      27.0      0.80\n",
       "4      Male  19                      White   24.0      27.0      0.80\n",
       "..      ...  ..                        ...    ...       ...       ...\n",
       "705  Female  19                      White   24.4      13.5      0.60\n",
       "706  Female  23                      Asian   24.4      14.7      0.63\n",
       "707    Male  23                Multiracial   22.0      30.0      0.60\n",
       "708    Male  19                      White   22.0      30.0      0.60\n",
       "709    Male  19  Black or African-American   22.0      30.0      0.60\n",
       "\n",
       "[710 rows x 6 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_features.loc[:, 'Age'] = [math.ceil((int(x.split('-')[1]) + int(x.split('-')[0]))/2) if '-' in x else int(x.strip('>')) for x in common_features['Age']]\n",
    "common_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(min(common_features['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(max(common_features['Age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "encoded_data = encoder.fit_transform(common_features[['Gender', 'Ethnicity']])\n",
    "\n",
    "# Create a DataFrame from the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Gender', 'Ethnicity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Ethnicity_American Indian or Alaskan Native</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Black or African-American</th>\n",
       "      <th>Ethnicity_Hispanic/Latino</th>\n",
       "      <th>Ethnicity_Multiracial</th>\n",
       "      <th>Ethnicity_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>24.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>19</td>\n",
       "      <td>24.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>23</td>\n",
       "      <td>24.4</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>23</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>19</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>19</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  T_atm  Humidity  Distance  Gender_Female  Gender_Male  \\\n",
       "0    46   24.0      28.0      0.80            0.0          1.0   \n",
       "1    36   24.0      26.0      0.80            1.0          0.0   \n",
       "2    26   24.0      26.0      0.80            1.0          0.0   \n",
       "3    26   24.0      27.0      0.80            1.0          0.0   \n",
       "4    19   24.0      27.0      0.80            0.0          1.0   \n",
       "..   ..    ...       ...       ...            ...          ...   \n",
       "705  19   24.4      13.5      0.60            1.0          0.0   \n",
       "706  23   24.4      14.7      0.63            1.0          0.0   \n",
       "707  23   22.0      30.0      0.60            0.0          1.0   \n",
       "708  19   22.0      30.0      0.60            0.0          1.0   \n",
       "709  19   22.0      30.0      0.60            0.0          1.0   \n",
       "\n",
       "     Ethnicity_American Indian or Alaskan Native  Ethnicity_Asian  \\\n",
       "0                                            0.0              0.0   \n",
       "1                                            0.0              0.0   \n",
       "2                                            0.0              0.0   \n",
       "3                                            0.0              0.0   \n",
       "4                                            0.0              0.0   \n",
       "..                                           ...              ...   \n",
       "705                                          0.0              0.0   \n",
       "706                                          0.0              1.0   \n",
       "707                                          0.0              0.0   \n",
       "708                                          0.0              0.0   \n",
       "709                                          0.0              0.0   \n",
       "\n",
       "     Ethnicity_Black or African-American  Ethnicity_Hispanic/Latino  \\\n",
       "0                                    0.0                        0.0   \n",
       "1                                    1.0                        0.0   \n",
       "2                                    0.0                        0.0   \n",
       "3                                    1.0                        0.0   \n",
       "4                                    0.0                        0.0   \n",
       "..                                   ...                        ...   \n",
       "705                                  0.0                        0.0   \n",
       "706                                  0.0                        0.0   \n",
       "707                                  0.0                        0.0   \n",
       "708                                  0.0                        0.0   \n",
       "709                                  1.0                        0.0   \n",
       "\n",
       "     Ethnicity_Multiracial  Ethnicity_White  \n",
       "0                      0.0              1.0  \n",
       "1                      0.0              0.0  \n",
       "2                      0.0              1.0  \n",
       "3                      0.0              0.0  \n",
       "4                      0.0              1.0  \n",
       "..                     ...              ...  \n",
       "705                    0.0              1.0  \n",
       "706                    0.0              0.0  \n",
       "707                    1.0              0.0  \n",
       "708                    0.0              1.0  \n",
       "709                    0.0              0.0  \n",
       "\n",
       "[710 rows x 12 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_features = common_features.drop(columns=['Gender', 'Ethnicity'])\n",
    "common_features = pd.concat([common_features, encoded_df], axis=1)\n",
    "\n",
    "common_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "columns_set = {\n",
    "    \n",
    "    'T_offset_' : [1,2,3,4],\n",
    "    'Max1R13_': [1,2,3,4],\n",
    "    'Max1L13_': [1,2,3,4],\n",
    "    'aveAllR13_':  [1,2,3,4],\n",
    "    'aveAllL13_': [1,2,3,4],\n",
    "    'T_RC_' : [1,2,3,4],\n",
    "    'T_RC_Dry_': [1,2,3,4],\n",
    "    'T_RC_Wet_': [1,2,3,4],\n",
    "    'T_RC_Max_': [1,2,3,4],\n",
    "    'T_LC_': [1,2,3,4],\n",
    "    'T_LC_Dry_': [1,2,3,4],\n",
    "    'T_LC_Wet_': [1,2,3,4],\n",
    "    'T_LC_Max_': [1,2,3,4],\n",
    "    'RCC_': [1,2,3,4],\n",
    "    'LCC_': [1,2,3,4],\n",
    "    'canthiMax_': [1,2,3,4],\n",
    "    'canthi4Max_': [1,2,3,4],\n",
    "    'T_FHCC_': [1,2,3,4],\n",
    "    'T_FHRC_': [1,2,3,4],\n",
    "    'T_FHLC_': [1,2,3,4],\n",
    "    'T_FHBC_': [1,2,3,4],\n",
    "    'T_FHTC_': [1,2,3,4],\n",
    "    'T_FH_Max_': [1,2,3,4],\n",
    "    'T_FHC_Max_': [1,2,3,4],\n",
    "    'T_Max_': [1,2,3,4],\n",
    "    'T_OR_': [1,2,3,4],\n",
    "    'T_OR_Max_': [1,2,3,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with NaN values filled with means for thermal data\n",
    "df_filled = pd.DataFrame()\n",
    "column_names = []\n",
    "for header, rounds in columns_set.items():\n",
    "    for roundd in rounds:\n",
    "        column_names.append(f'{header}{roundd}')\n",
    "    \n",
    "thermal_info = df[column_names]\n",
    "thermal_info = thermal_info.fillna(thermal_info.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column wise mean of 4 rounds\n",
    "new_mean_dataframe = pd.DataFrame()\n",
    "\n",
    "for header, rounds in columns_set.items():\n",
    "    column_names = [f'{header}{roundd}' for roundd in rounds]\n",
    "    new_mean_dataframe[f'{header}mean'] = thermal_info[column_names].mean(axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T_offset_mean</th>\n",
       "      <th>Max1R13_mean</th>\n",
       "      <th>Max1L13_mean</th>\n",
       "      <th>aveAllR13_mean</th>\n",
       "      <th>aveAllL13_mean</th>\n",
       "      <th>T_RC_mean</th>\n",
       "      <th>T_RC_Dry_mean</th>\n",
       "      <th>T_RC_Wet_mean</th>\n",
       "      <th>T_RC_Max_mean</th>\n",
       "      <th>T_LC_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Ethnicity_American Indian or Alaskan Native</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Black or African-American</th>\n",
       "      <th>Ethnicity_Hispanic/Latino</th>\n",
       "      <th>Ethnicity_Multiracial</th>\n",
       "      <th>Ethnicity_White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7025</td>\n",
       "      <td>35.0300</td>\n",
       "      <td>35.3775</td>\n",
       "      <td>34.4000</td>\n",
       "      <td>34.9175</td>\n",
       "      <td>34.9850</td>\n",
       "      <td>34.9850</td>\n",
       "      <td>34.7625</td>\n",
       "      <td>35.0325</td>\n",
       "      <td>35.3375</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7800</td>\n",
       "      <td>34.5500</td>\n",
       "      <td>34.5200</td>\n",
       "      <td>33.9300</td>\n",
       "      <td>34.2250</td>\n",
       "      <td>34.7100</td>\n",
       "      <td>34.6325</td>\n",
       "      <td>34.6400</td>\n",
       "      <td>34.7425</td>\n",
       "      <td>34.5600</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8625</td>\n",
       "      <td>35.6525</td>\n",
       "      <td>35.5175</td>\n",
       "      <td>34.2775</td>\n",
       "      <td>34.8000</td>\n",
       "      <td>35.6850</td>\n",
       "      <td>35.6675</td>\n",
       "      <td>35.6150</td>\n",
       "      <td>35.7175</td>\n",
       "      <td>35.5025</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9300</td>\n",
       "      <td>35.2225</td>\n",
       "      <td>35.6125</td>\n",
       "      <td>34.3850</td>\n",
       "      <td>35.2475</td>\n",
       "      <td>35.2075</td>\n",
       "      <td>35.2000</td>\n",
       "      <td>35.1175</td>\n",
       "      <td>35.2250</td>\n",
       "      <td>35.5950</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8950</td>\n",
       "      <td>35.5450</td>\n",
       "      <td>35.6650</td>\n",
       "      <td>34.9100</td>\n",
       "      <td>35.3675</td>\n",
       "      <td>35.6025</td>\n",
       "      <td>35.4750</td>\n",
       "      <td>35.5700</td>\n",
       "      <td>35.6400</td>\n",
       "      <td>35.6400</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.9325</td>\n",
       "      <td>35.4800</td>\n",
       "      <td>35.5300</td>\n",
       "      <td>34.9000</td>\n",
       "      <td>34.9900</td>\n",
       "      <td>35.5650</td>\n",
       "      <td>35.5650</td>\n",
       "      <td>35.1350</td>\n",
       "      <td>35.6300</td>\n",
       "      <td>35.5325</td>\n",
       "      <td>...</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.8550</td>\n",
       "      <td>35.6550</td>\n",
       "      <td>35.5325</td>\n",
       "      <td>35.1925</td>\n",
       "      <td>35.2075</td>\n",
       "      <td>35.6125</td>\n",
       "      <td>35.6000</td>\n",
       "      <td>35.4850</td>\n",
       "      <td>35.6550</td>\n",
       "      <td>35.5275</td>\n",
       "      <td>...</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.9700</td>\n",
       "      <td>36.7325</td>\n",
       "      <td>36.4600</td>\n",
       "      <td>36.2225</td>\n",
       "      <td>36.1150</td>\n",
       "      <td>36.7175</td>\n",
       "      <td>36.7150</td>\n",
       "      <td>36.6400</td>\n",
       "      <td>36.7350</td>\n",
       "      <td>36.4350</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>1.0725</td>\n",
       "      <td>36.9450</td>\n",
       "      <td>37.0675</td>\n",
       "      <td>36.3825</td>\n",
       "      <td>36.4825</td>\n",
       "      <td>36.9250</td>\n",
       "      <td>36.9200</td>\n",
       "      <td>36.8200</td>\n",
       "      <td>36.9475</td>\n",
       "      <td>37.0500</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1.0750</td>\n",
       "      <td>36.9825</td>\n",
       "      <td>35.6625</td>\n",
       "      <td>35.8875</td>\n",
       "      <td>35.3825</td>\n",
       "      <td>37.1000</td>\n",
       "      <td>36.9700</td>\n",
       "      <td>37.0350</td>\n",
       "      <td>37.1375</td>\n",
       "      <td>37.0250</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     T_offset_mean  Max1R13_mean  Max1L13_mean  aveAllR13_mean  \\\n",
       "0           0.7025       35.0300       35.3775         34.4000   \n",
       "1           0.7800       34.5500       34.5200         33.9300   \n",
       "2           0.8625       35.6525       35.5175         34.2775   \n",
       "3           0.9300       35.2225       35.6125         34.3850   \n",
       "4           0.8950       35.5450       35.6650         34.9100   \n",
       "..             ...           ...           ...             ...   \n",
       "705         0.9325       35.4800       35.5300         34.9000   \n",
       "706         0.8550       35.6550       35.5325         35.1925   \n",
       "707         0.9700       36.7325       36.4600         36.2225   \n",
       "708         1.0725       36.9450       37.0675         36.3825   \n",
       "709         1.0750       36.9825       35.6625         35.8875   \n",
       "\n",
       "     aveAllL13_mean  T_RC_mean  T_RC_Dry_mean  T_RC_Wet_mean  T_RC_Max_mean  \\\n",
       "0           34.9175    34.9850        34.9850        34.7625        35.0325   \n",
       "1           34.2250    34.7100        34.6325        34.6400        34.7425   \n",
       "2           34.8000    35.6850        35.6675        35.6150        35.7175   \n",
       "3           35.2475    35.2075        35.2000        35.1175        35.2250   \n",
       "4           35.3675    35.6025        35.4750        35.5700        35.6400   \n",
       "..              ...        ...            ...            ...            ...   \n",
       "705         34.9900    35.5650        35.5650        35.1350        35.6300   \n",
       "706         35.2075    35.6125        35.6000        35.4850        35.6550   \n",
       "707         36.1150    36.7175        36.7150        36.6400        36.7350   \n",
       "708         36.4825    36.9250        36.9200        36.8200        36.9475   \n",
       "709         35.3825    37.1000        36.9700        37.0350        37.1375   \n",
       "\n",
       "     T_LC_mean  ...  Humidity  Distance  Gender_Female  Gender_Male  \\\n",
       "0      35.3375  ...      28.0      0.80            0.0          1.0   \n",
       "1      34.5600  ...      26.0      0.80            1.0          0.0   \n",
       "2      35.5025  ...      26.0      0.80            1.0          0.0   \n",
       "3      35.5950  ...      27.0      0.80            1.0          0.0   \n",
       "4      35.6400  ...      27.0      0.80            0.0          1.0   \n",
       "..         ...  ...       ...       ...            ...          ...   \n",
       "705    35.5325  ...      13.5      0.60            1.0          0.0   \n",
       "706    35.5275  ...      14.7      0.63            1.0          0.0   \n",
       "707    36.4350  ...      30.0      0.60            0.0          1.0   \n",
       "708    37.0500  ...      30.0      0.60            0.0          1.0   \n",
       "709    37.0250  ...      30.0      0.60            0.0          1.0   \n",
       "\n",
       "     Ethnicity_American Indian or Alaskan Native  Ethnicity_Asian  \\\n",
       "0                                            0.0              0.0   \n",
       "1                                            0.0              0.0   \n",
       "2                                            0.0              0.0   \n",
       "3                                            0.0              0.0   \n",
       "4                                            0.0              0.0   \n",
       "..                                           ...              ...   \n",
       "705                                          0.0              0.0   \n",
       "706                                          0.0              1.0   \n",
       "707                                          0.0              0.0   \n",
       "708                                          0.0              0.0   \n",
       "709                                          0.0              0.0   \n",
       "\n",
       "     Ethnicity_Black or African-American  Ethnicity_Hispanic/Latino  \\\n",
       "0                                    0.0                        0.0   \n",
       "1                                    1.0                        0.0   \n",
       "2                                    0.0                        0.0   \n",
       "3                                    1.0                        0.0   \n",
       "4                                    0.0                        0.0   \n",
       "..                                   ...                        ...   \n",
       "705                                  0.0                        0.0   \n",
       "706                                  0.0                        0.0   \n",
       "707                                  0.0                        0.0   \n",
       "708                                  0.0                        0.0   \n",
       "709                                  1.0                        0.0   \n",
       "\n",
       "     Ethnicity_Multiracial  Ethnicity_White  \n",
       "0                      0.0              1.0  \n",
       "1                      0.0              0.0  \n",
       "2                      0.0              1.0  \n",
       "3                      0.0              0.0  \n",
       "4                      0.0              1.0  \n",
       "..                     ...              ...  \n",
       "705                    0.0              1.0  \n",
       "706                    0.0              0.0  \n",
       "707                    1.0              0.0  \n",
       "708                    0.0              1.0  \n",
       "709                    0.0              0.0  \n",
       "\n",
       "[710 rows x 39 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.concat([new_mean_dataframe, common_features], axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T_offset_mean', 'Max1R13_mean', 'Max1L13_mean', 'aveAllR13_mean',\n",
       "       'aveAllL13_mean', 'T_RC_mean', 'T_RC_Dry_mean', 'T_RC_Wet_mean',\n",
       "       'T_RC_Max_mean', 'T_LC_mean', 'T_LC_Dry_mean', 'T_LC_Wet_mean',\n",
       "       'T_LC_Max_mean', 'RCC_mean', 'LCC_mean', 'canthiMax_mean',\n",
       "       'canthi4Max_mean', 'T_FHCC_mean', 'T_FHRC_mean', 'T_FHLC_mean',\n",
       "       'T_FHBC_mean', 'T_FHTC_mean', 'T_FH_Max_mean', 'T_FHC_Max_mean',\n",
       "       'T_Max_mean', 'T_OR_mean', 'T_OR_Max_mean', 'Age', 'T_atm', 'Humidity',\n",
       "       'Distance', 'Gender_Female', 'Gender_Male',\n",
       "       'Ethnicity_American Indian or Alaskan Native', 'Ethnicity_Asian',\n",
       "       'Ethnicity_Black or African-American', 'Ethnicity_Hispanic/Latino',\n",
       "       'Ethnicity_Multiracial', 'Ethnicity_White'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['aveAllL13_mean', 'T_offset_mean', 'T_FH_Max_mean', 'T_Max_mean', 'T_OR_Max_mean', 'T_RC_mean', 'T_atm', 'Max1R13_mean', 'Max1L13_mean', 'T_FHLC_mean', 'T_LC_Dry_mean', 'canthi4Max_mean', 'T_FHC_Max_mean', 'Distance', 'T_FHBC_mean']\n",
    "X_train_selected_features = X_train[columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_original = scaler.fit_transform(X_train_selected_features)\n",
    "X_train_original = pd.DataFrame(X_train_original, columns=X_train_selected_features.columns)\n",
    "\n",
    "pca = PCA(n_components = 5)\n",
    "X_train_original = pca.fit_transform(X_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_original = X_train.iloc[:, 1:].values.astype(float)\n",
    "#y_train_original = y_train.iloc[:].values.astype(float)\n",
    "y_train_original = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Model creation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanTrivialSystem:\n",
    "    def __init__(self):\n",
    "        self_output_value = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.output_value = sum(y) / len(y)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return [self.output_value] * len(X)       \n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1NN\n",
    "\n",
    "class NearestNeighbor1(object):\n",
    "    def __init__(self, p_val: int = 2) -> None:\n",
    "        self.knn_reg = KNeighborsRegressor(\n",
    "            n_neighbors=1,\n",
    "            weights='uniform',\n",
    "            algorithm='auto',\n",
    "            leaf_size=30,\n",
    "            p=p_val,\n",
    "            metric='minkowski',\n",
    "            n_jobs=None\n",
    "        )\n",
    "\n",
    "        self.cv_scores = None\n",
    "\n",
    "    def fit(self, X, y) -> None:\n",
    "        self.knn_reg.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.knn_reg.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.knn_reg.score(X, y)\n",
    "    \n",
    "    def cross_validate(self, X, y, n_splits=5, shuffle=True, random_state=42):\n",
    "        cross_val = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "        self.cv_scores = cross_val_score(self.knn_reg, X, y, cv=cross_val)\n",
    "        return self.cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation rmse: 0.2830881291910502\n",
      "validation rmse: 0.3675179511866662\n",
      "validation rmse: 0.3929058411375423\n",
      "validation rmse: 0.38756719847444976\n",
      "validation rmse: 0.33082388735465307\n",
      "validation rmse: 0.2787621447279623\n",
      "validation rmse: 0.37481476906748373\n",
      "validation rmse: 0.3767551518485774\n",
      "validation rmse: 0.39973949850704904\n",
      "validation rmse: 0.33767999315591435\n",
      "validation rmse: 0.38739053753470637\n",
      "validation rmse: 0.32260103622187636\n",
      "validation rmse: 0.32315409857925614\n",
      "validation rmse: 0.43907370028927223\n",
      "validation rmse: 0.3350906060839584\n",
      "validation rmse: 0.359960315272988\n",
      "validation rmse: 0.2937443008565684\n",
      "validation rmse: 0.3153229636873459\n",
      "validation rmse: 0.36974894957833915\n",
      "validation rmse: 0.3149829927381569\n",
      "mae_trains: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "mean training mae: 0.0\n",
      "mse_trains: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "mean training mse: 0.0\n",
      "rmse_trains: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "mean training rmse: 0.0\n",
      "mae_vals: [0.23055555555555507, 0.3013888888888893, 0.31805555555555504, 0.30138888888888893, 0.25833333333333286, 0.22361111111111104, 0.30138888888888854, 0.2805555555555552, 0.32361111111111146, 0.2638888888888889, 0.29571428571428576, 0.244285714285714, 0.25428571428571445, 0.3299999999999993, 0.25714285714285773, 0.28285714285714303, 0.22285714285714237, 0.21999999999999928, 0.2999999999999992, 0.2385714285714286]\n",
      "mean validation mae: 0.272424603174603\n",
      "mse_vals: [0.08013888888888872, 0.13506944444444474, 0.15437499999999962, 0.15020833333333353, 0.10944444444444419, 0.07770833333333341, 0.14048611111111117, 0.1419444444444446, 0.15979166666666705, 0.11402777777777835, 0.15007142857142874, 0.10407142857142837, 0.1044285714285716, 0.19278571428571364, 0.11228571428571459, 0.12957142857142892, 0.08628571428571417, 0.09942857142857127, 0.1367142857142852, 0.0992142857142858]\n",
      "mean validation mse: 0.12390257936507938\n",
      "rmse_vals: [0.2830881291910502, 0.3675179511866662, 0.3929058411375423, 0.38756719847444976, 0.33082388735465307, 0.2787621447279623, 0.37481476906748373, 0.3767551518485774, 0.39973949850704904, 0.33767999315591435, 0.38739053753470637, 0.32260103622187636, 0.32315409857925614, 0.43907370028927223, 0.3350906060839584, 0.359960315272988, 0.2937443008565684, 0.3153229636873459, 0.36974894957833915, 0.3149829927381569]\n",
      "mean validation rmse: 0.34953620327469087\n"
     ]
    }
   ],
   "source": [
    "# 1NN 20-fold cross validation (on normalized dataset)\n",
    "\n",
    "num_splits = 20\n",
    "kf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "mae_trains = []\n",
    "mse_trains = []\n",
    "rmse_trains = []\n",
    "\n",
    "mae_vals = []\n",
    "mse_vals = []\n",
    "rmse_vals = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_original):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    NN = NearestNeighbor1()\n",
    "    NN.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    train_pred = NN.predict(X_train_fold)\n",
    "    val_pred = NN.predict(X_val_fold)\n",
    "\n",
    "    # training metrics\n",
    "    mae_train = mean_absolute_error(y_train_fold, train_pred)\n",
    "    mae_trains.append(mae_train)\n",
    "    mse_train = mean_squared_error(y_train_fold, train_pred, squared=True)\n",
    "    mse_trains.append(mse_train)\n",
    "    rmse_train = mean_squared_error(y_train_fold, train_pred, squared=False)\n",
    "    rmse_trains.append(rmse_train)\n",
    "    \n",
    "    # validation metrics\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_pred)\n",
    "    mae_vals.append(mae_val)\n",
    "    mse_val = mean_squared_error(y_val_fold, val_pred, squared=True)\n",
    "    mse_vals.append(mse_val)\n",
    "    rmse_val = mean_squared_error(y_val_fold, val_pred, squared=False)\n",
    "    rmse_vals.append(rmse_val)\n",
    "    \n",
    "    print(f'validation rmse: {rmse_val}')\n",
    "\n",
    "mse_train_mean = sum(mse_trains)/len(mse_trains)\n",
    "rmse_train_mean = sum(rmse_trains)/len(rmse_trains)\n",
    "mae_train_mean = sum(mae_trains)/len(mae_trains) \n",
    "\n",
    "mse_val_mean = sum(mse_vals)/len(mse_vals)\n",
    "rmse_val_mean = sum(rmse_vals)/len(rmse_vals)\n",
    "mae_val_mean = sum(mae_vals)/len(mae_vals)\n",
    "\n",
    "print(f'mae_trains: {mae_trains}')\n",
    "print(f'mean training mae: {mae_train_mean}')\n",
    "print(f'mse_trains: {mse_trains}')\n",
    "print(f'mean training mse: {mse_train_mean}')\n",
    "print(f'rmse_trains: {rmse_trains}')\n",
    "print(f'mean training rmse: {rmse_train_mean}')\n",
    "\n",
    "print(f'mae_vals: {mae_vals}')\n",
    "print(f'mean validation mae: {mae_val_mean}')\n",
    "print(f'mse_vals: {mse_vals}')\n",
    "print(f'mean validation mse: {mse_val_mean}')\n",
    "print(f'rmse_vals: {rmse_vals}')\n",
    "print(f'mean validation rmse: {rmse_val_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2376469760464194\n",
      "0.48723527683216333\n",
      "0.31586001041461065\n",
      "0.23845573174965323\n",
      "0.4839243646729258\n",
      "0.3160925411624781\n"
     ]
    }
   ],
   "source": [
    "# Trivial System\n",
    "\n",
    "train_rmse = []\n",
    "train_mse = []\n",
    "train_mae = []\n",
    "\n",
    "val_rmse = []\n",
    "val_mse = []\n",
    "val_mae = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_original):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_original[train_index], y_train_original[val_index]\n",
    "    \n",
    "\n",
    "    trivial = MeanTrivialSystem()\n",
    "    trivial.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = trivial.predict(X_train_fold)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "    \n",
    "    train_mse.append(mse_train)\n",
    "    train_rmse.append(rmse_train)\n",
    "    train_mae.append(mae_train)\n",
    "    \n",
    "    val_predictions = trivial.predict(X_val_fold)\n",
    "\n",
    "    mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "    \n",
    "    \n",
    "    val_mse.append(mse_val)\n",
    "    val_rmse.append(rmse_val)\n",
    "    val_mae.append(mae_val)\n",
    "\n",
    "print(np.mean(train_mse))       \n",
    "print(np.mean(train_rmse))   \n",
    "print(np.mean(train_mae))   \n",
    "\n",
    "print(np.mean(val_mse))       \n",
    "print(np.mean(val_rmse))   \n",
    "print(np.mean(val_mae))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07040407830673309\n",
      "0.26531675103543173\n",
      "0.20676974809391013\n",
      "0.07273528501550414\n",
      "0.2693804343266899\n",
      "0.20947244945350615\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "\n",
    "train_rmse = []\n",
    "train_mse = []\n",
    "train_mae = []\n",
    "\n",
    "val_rmse = []\n",
    "val_mse = []\n",
    "val_mae = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_original):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_original[train_index], y_train_original[val_index]\n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    train_predictions = linear_reg.predict(X_train_fold)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "    \n",
    "    train_mse.append(mse_train)\n",
    "    train_rmse.append(rmse_train)\n",
    "    train_mae.append(mae_train)\n",
    "    \n",
    "    val_predictions = linear_reg.predict(X_val_fold)\n",
    "\n",
    "    mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "    rmse_val = np.sqrt(mse_val)\n",
    "    mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "    \n",
    "    \n",
    "    val_mse.append(mse_val)\n",
    "    val_rmse.append(rmse_val)\n",
    "    val_mae.append(mae_val)\n",
    "\n",
    "print(np.mean(train_mse))       \n",
    "print(np.mean(train_rmse))   \n",
    "print(np.mean(train_mae))   \n",
    "\n",
    "print(np.mean(val_mse))       \n",
    "print(np.mean(val_rmse))   \n",
    "print(np.mean(val_mae))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of support vectors: 390\n",
      "Degrees of freedom: 320\n",
      "0.05739353657831085\n",
      "0.23949265460285413\n",
      "0.1829166110362336\n",
      "0.06178969594761895\n",
      "0.24741739639183083\n",
      "0.18915654176095456\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Regressor\n",
    "\n",
    "def supportvectorreg(X_train_original, y_train_original, kernel='linear', C=10, gamma='auto'):\n",
    "    \n",
    "    train_rmse = []\n",
    "    train_mse = []\n",
    "    train_mae = []\n",
    "\n",
    "    val_rmse = []\n",
    "    val_mse = []\n",
    "    val_mae = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_original):\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train_original[train_index], X_train_original[val_index]\n",
    "        y_train_fold, y_val_fold = y_train_original[train_index], y_train_original[val_index]\n",
    "        \n",
    "        if kernel == 'linear':\n",
    "            clf = svm.SVR(kernel=kernel, C=C)\n",
    "        elif kernel == 'rbf':\n",
    "            clf = svm.SVR(kernel=kernel, C=C, gamma=gamma)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid kernel type.\")\n",
    "\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        train_predictions = clf.predict(X_train_fold)\n",
    "        \n",
    "        mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "        rmse_train = np.sqrt(mse_train)\n",
    "        mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "        \n",
    "        train_mse.append(mse_train)\n",
    "        train_rmse.append(rmse_train)\n",
    "        train_mae.append(mae_train)\n",
    "        \n",
    "        val_predictions = clf.predict(X_val_fold)\n",
    "\n",
    "        mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "        rmse_val = np.sqrt(mse_val)\n",
    "        mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "        \n",
    "        \n",
    "        val_mse.append(mse_val)\n",
    "        val_rmse.append(rmse_val)\n",
    "        val_mae.append(mae_val)\n",
    "        \n",
    "    # Get the number of support vectors\n",
    "    n_support_vectors = np.sum(clf.n_support_)\n",
    "    degrees_of_freedom = len(X_train_original) - n_support_vectors\n",
    "    \n",
    "    print(\"Number of support vectors:\", n_support_vectors)\n",
    "    print(\"Degrees of freedom:\", degrees_of_freedom)\n",
    "    print(np.mean(train_mse))       \n",
    "    print(np.mean(train_rmse))   \n",
    "    print(np.mean(train_mae))   \n",
    "\n",
    "    print(np.mean(val_mse))       \n",
    "    print(np.mean(val_rmse))   \n",
    "    print(np.mean(val_mae))  \n",
    "\n",
    "supportvectorreg(X_train_original, y_train_original, kernel='rbf')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of freedom for order: 1 = 10\n",
      "0.06661452139045329\n",
      "0.2580657322184529\n",
      "0.20024218169534858\n",
      "0.412626664741128\n",
      "0.4788858834699166\n",
      "0.2260451077938827\n",
      "Degree of freedom for order: 2 = 10\n",
      "0.05761386062014502\n",
      "0.23996118844829115\n",
      "0.18589156663607237\n",
      "0.5345316981497183\n",
      "0.5090686301000777\n",
      "0.2153596803874734\n",
      "Degree of freedom for order: 3 = 10\n",
      "0.05778229062937549\n",
      "0.2402936163114368\n",
      "0.18621425917287582\n",
      "0.09008869883497302\n",
      "0.28880141966402223\n",
      "0.1972114672049303\n",
      "Degree of freedom for order: 4 = 10\n",
      "0.059297962982124905\n",
      "0.2434155542974974\n",
      "0.18849058861757134\n",
      "0.06585634985371161\n",
      "0.254938002930977\n",
      "0.19530079519516005\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "def polynomial_regression(X_train,y_train, orders):\n",
    "  \n",
    "    for order in range(1,orders+1):\n",
    "      \n",
    "        train_rmse = []\n",
    "        train_mse = []\n",
    "        train_mae = []\n",
    "\n",
    "        val_rmse = []\n",
    "        val_mse = []\n",
    "        val_mae = []\n",
    "        \n",
    "        X_train_norm = scaler.fit_transform(X_train)\n",
    "        polynomial = PolynomialFeatures(degree=order, include_bias=True)\n",
    "        X_train_polynomial = polynomial.fit_transform(X_train_norm)\n",
    "        \n",
    "        pca = PCA(n_components = 10)\n",
    "        X_train_polynomial = pca.fit_transform(X_train_polynomial) \n",
    "        \n",
    "        print(f'Degree of freedom for order: {order} = {X_train_polynomial.shape[1]}')\n",
    "        \n",
    "        \n",
    "        for train_index, val_index in kf.split(X_train_original):\n",
    "            \n",
    "            X_train_fold, X_val_fold = X_train_polynomial[train_index], X_train_polynomial[val_index]\n",
    "            y_train_fold, y_val_fold = y_train_original[train_index], y_train_original[val_index]\n",
    "\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            \n",
    "            train_predictions = model.predict(X_train_fold)\n",
    "            \n",
    "            mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "            rmse_train = np.sqrt(mse_train)\n",
    "            mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "            \n",
    "            train_mse.append(mse_train)\n",
    "            train_rmse.append(rmse_train)\n",
    "            train_mae.append(mae_train)\n",
    "            \n",
    "            val_predictions = model.predict(X_val_fold)\n",
    "\n",
    "            mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "            rmse_val = np.sqrt(mse_val)\n",
    "            mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "        \n",
    "        \n",
    "            val_mse.append(mse_val)\n",
    "            val_rmse.append(rmse_val)\n",
    "            val_mae.append(mae_val)\n",
    "\n",
    "        print(np.mean(train_mse))       \n",
    "        print(np.mean(train_rmse))   \n",
    "        print(np.mean(train_mae))   \n",
    "\n",
    "        print(np.mean(val_mse))       \n",
    "        print(np.mean(val_rmse))   \n",
    "        print(np.mean(val_mae))  \n",
    "        \n",
    "\n",
    "polynomial_regression(X_train_selected_features, y_train, 4)\n",
    "\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010393141461267793\n",
      "0.10191212353502728\n",
      "0.07567411971831038\n",
      "0.07130551443661963\n",
      "0.2660787210189588\n",
      "0.2014049295774641\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "def random_forest(X_train,y_train):\n",
    "  \n",
    "      \n",
    "        train_rmse = []\n",
    "        train_mse = []\n",
    "        train_mae = []\n",
    "\n",
    "        val_rmse = []\n",
    "        val_mse = []\n",
    "        val_mae = []\n",
    "        \n",
    "        \n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "            \n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            \n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "            train_predictions = model.predict(X_train_fold)\n",
    "            \n",
    "            mse_train = mean_squared_error(y_train_fold, train_predictions)\n",
    "            rmse_train = np.sqrt(mse_train)\n",
    "            mae_train = mean_absolute_error(y_train_fold, train_predictions)\n",
    "            \n",
    "            train_mse.append(mse_train)\n",
    "            train_rmse.append(rmse_train)\n",
    "            train_mae.append(mae_train)\n",
    "            \n",
    "            val_predictions = model.predict(X_val_fold)\n",
    "\n",
    "            mse_val = mean_squared_error(y_val_fold, val_predictions)\n",
    "            rmse_val = np.sqrt(mse_val)\n",
    "            mae_val = mean_absolute_error(y_val_fold, val_predictions)\n",
    "        \n",
    "        \n",
    "            val_mse.append(mse_val)\n",
    "            val_rmse.append(rmse_val)\n",
    "            val_mae.append(mae_val)\n",
    "\n",
    "        print(np.mean(train_mse))       \n",
    "        print(np.mean(train_rmse))   \n",
    "        print(np.mean(train_mae))   \n",
    "\n",
    "        print(np.mean(val_mse))       \n",
    "        print(np.mean(val_rmse))   \n",
    "        print(np.mean(val_mae)) \n",
    "        \n",
    "\n",
    "random_forest(X_train_original, y_train_original)\n",
    "\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FLIR_groups1and2_test.csv', skiprows=2)\n",
    "df = df.dropna(axis=1, how='all')\n",
    "X_test = df.iloc[:, :-1]\n",
    "y_test_original = df.loc[:, 'aveOralM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "columns_set = {\n",
    "    \n",
    "    'T_offset_' : [1,2,3,4],\n",
    "    'Max1R13_': [1,2,3,4],\n",
    "    'Max1L13_': [1,2,3,4],\n",
    "    'aveAllR13_':  [1,2,3,4],\n",
    "    'aveAllL13_': [1,2,3,4],\n",
    "    'T_RC_' : [1,2,3,4],\n",
    "    'T_RC_Dry_': [1,2,3,4],\n",
    "    'T_RC_Wet_': [1,2,3,4],\n",
    "    'T_RC_Max_': [1,2,3,4],\n",
    "    'T_LC_': [1,2,3,4],\n",
    "    'T_LC_Dry_': [1,2,3,4],\n",
    "    'T_LC_Wet_': [1,2,3,4],\n",
    "    'T_LC_Max_': [1,2,3,4],\n",
    "    'RCC_': [1,2,3,4],\n",
    "    'LCC_': [1,2,3,4],\n",
    "    'canthiMax_': [1,2,3,4],\n",
    "    'canthi4Max_': [1,2,3,4],\n",
    "    'T_FHCC_': [1,2,3,4],\n",
    "    'T_FHRC_': [1,2,3,4],\n",
    "    'T_FHLC_': [1,2,3,4],\n",
    "    'T_FHBC_': [1,2,3,4],\n",
    "    'T_FHTC_': [1,2,3,4],\n",
    "    'T_FH_Max_': [1,2,3,4],\n",
    "    'T_FHC_Max_': [1,2,3,4],\n",
    "    'T_Max_': [1,2,3,4],\n",
    "    'T_OR_': [1,2,3,4],\n",
    "    'T_OR_Max_': [1,2,3,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows with NaN values filled with means for thermal data\n",
    "df_filled = pd.DataFrame()\n",
    "column_names = []\n",
    "for header, rounds in columns_set.items():\n",
    "    for roundd in rounds:\n",
    "        column_names.append(f'{header}{roundd}')\n",
    "    \n",
    "thermal_info = df[column_names]\n",
    "thermal_info = thermal_info.fillna(thermal_info.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column wise mean of 4 rounds\n",
    "new_mean_dataframe = pd.DataFrame()\n",
    "\n",
    "for header, rounds in columns_set.items():\n",
    "    column_names = [f'{header}{roundd}' for roundd in rounds]\n",
    "    new_mean_dataframe[f'{header}mean'] = thermal_info[column_names].mean(axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T_offset_mean</th>\n",
       "      <th>Max1R13_mean</th>\n",
       "      <th>Max1L13_mean</th>\n",
       "      <th>aveAllR13_mean</th>\n",
       "      <th>aveAllL13_mean</th>\n",
       "      <th>T_RC_mean</th>\n",
       "      <th>T_RC_Dry_mean</th>\n",
       "      <th>T_RC_Wet_mean</th>\n",
       "      <th>T_RC_Max_mean</th>\n",
       "      <th>T_LC_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>T_FHBC_mean</th>\n",
       "      <th>T_FHTC_mean</th>\n",
       "      <th>T_FH_Max_mean</th>\n",
       "      <th>T_FHC_Max_mean</th>\n",
       "      <th>T_Max_mean</th>\n",
       "      <th>T_OR_mean</th>\n",
       "      <th>T_OR_Max_mean</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.067500</td>\n",
       "      <td>35.600000</td>\n",
       "      <td>35.332500</td>\n",
       "      <td>35.305000</td>\n",
       "      <td>35.002500</td>\n",
       "      <td>35.585000</td>\n",
       "      <td>35.585000</td>\n",
       "      <td>35.395000</td>\n",
       "      <td>35.600000</td>\n",
       "      <td>35.390000</td>\n",
       "      <td>...</td>\n",
       "      <td>34.79250</td>\n",
       "      <td>34.837500</td>\n",
       "      <td>35.582500</td>\n",
       "      <td>35.3775</td>\n",
       "      <td>36.072500</td>\n",
       "      <td>35.970000</td>\n",
       "      <td>36.017500</td>\n",
       "      <td>22.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555000</td>\n",
       "      <td>36.112500</td>\n",
       "      <td>36.237500</td>\n",
       "      <td>35.535000</td>\n",
       "      <td>35.810000</td>\n",
       "      <td>36.187500</td>\n",
       "      <td>36.187500</td>\n",
       "      <td>35.637500</td>\n",
       "      <td>36.200000</td>\n",
       "      <td>36.205000</td>\n",
       "      <td>...</td>\n",
       "      <td>35.42250</td>\n",
       "      <td>35.307500</td>\n",
       "      <td>36.187500</td>\n",
       "      <td>35.6525</td>\n",
       "      <td>36.720000</td>\n",
       "      <td>36.677500</td>\n",
       "      <td>36.720000</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.767500</td>\n",
       "      <td>37.620000</td>\n",
       "      <td>37.130000</td>\n",
       "      <td>37.162500</td>\n",
       "      <td>36.530000</td>\n",
       "      <td>37.695000</td>\n",
       "      <td>37.567500</td>\n",
       "      <td>37.690000</td>\n",
       "      <td>37.740000</td>\n",
       "      <td>37.140000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.53000</td>\n",
       "      <td>35.420000</td>\n",
       "      <td>37.510000</td>\n",
       "      <td>36.9975</td>\n",
       "      <td>37.975000</td>\n",
       "      <td>37.595000</td>\n",
       "      <td>37.645000</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.850000</td>\n",
       "      <td>35.490000</td>\n",
       "      <td>35.627500</td>\n",
       "      <td>34.865000</td>\n",
       "      <td>34.962500</td>\n",
       "      <td>35.490000</td>\n",
       "      <td>35.482500</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>35.517500</td>\n",
       "      <td>35.622500</td>\n",
       "      <td>...</td>\n",
       "      <td>34.17750</td>\n",
       "      <td>33.882500</td>\n",
       "      <td>34.830000</td>\n",
       "      <td>34.5625</td>\n",
       "      <td>36.107500</td>\n",
       "      <td>35.897500</td>\n",
       "      <td>35.970000</td>\n",
       "      <td>24.1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.877500</td>\n",
       "      <td>34.735000</td>\n",
       "      <td>34.660000</td>\n",
       "      <td>34.102500</td>\n",
       "      <td>34.185000</td>\n",
       "      <td>34.747500</td>\n",
       "      <td>34.747500</td>\n",
       "      <td>34.377500</td>\n",
       "      <td>34.790000</td>\n",
       "      <td>34.672500</td>\n",
       "      <td>...</td>\n",
       "      <td>33.55750</td>\n",
       "      <td>33.200000</td>\n",
       "      <td>34.565000</td>\n",
       "      <td>34.5650</td>\n",
       "      <td>35.552500</td>\n",
       "      <td>35.395000</td>\n",
       "      <td>35.427500</td>\n",
       "      <td>24.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1.222500</td>\n",
       "      <td>35.642500</td>\n",
       "      <td>35.652500</td>\n",
       "      <td>34.857500</td>\n",
       "      <td>35.005000</td>\n",
       "      <td>35.752500</td>\n",
       "      <td>35.657500</td>\n",
       "      <td>35.737500</td>\n",
       "      <td>35.775000</td>\n",
       "      <td>35.802500</td>\n",
       "      <td>...</td>\n",
       "      <td>35.13750</td>\n",
       "      <td>35.275000</td>\n",
       "      <td>35.852500</td>\n",
       "      <td>35.7475</td>\n",
       "      <td>36.067500</td>\n",
       "      <td>35.677500</td>\n",
       "      <td>35.710000</td>\n",
       "      <td>25.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1.467500</td>\n",
       "      <td>35.982500</td>\n",
       "      <td>35.757500</td>\n",
       "      <td>35.427500</td>\n",
       "      <td>35.197500</td>\n",
       "      <td>35.970000</td>\n",
       "      <td>35.950000</td>\n",
       "      <td>35.862500</td>\n",
       "      <td>36.007500</td>\n",
       "      <td>35.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>35.20750</td>\n",
       "      <td>35.070000</td>\n",
       "      <td>35.765000</td>\n",
       "      <td>35.5525</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>36.452500</td>\n",
       "      <td>36.490000</td>\n",
       "      <td>25.7</td>\n",
       "      <td>50.8</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.130000</td>\n",
       "      <td>36.407500</td>\n",
       "      <td>36.340000</td>\n",
       "      <td>35.870000</td>\n",
       "      <td>35.960000</td>\n",
       "      <td>36.410000</td>\n",
       "      <td>36.362500</td>\n",
       "      <td>36.365000</td>\n",
       "      <td>36.447500</td>\n",
       "      <td>36.302500</td>\n",
       "      <td>...</td>\n",
       "      <td>35.36750</td>\n",
       "      <td>35.342500</td>\n",
       "      <td>36.375000</td>\n",
       "      <td>35.7100</td>\n",
       "      <td>36.535000</td>\n",
       "      <td>35.965000</td>\n",
       "      <td>35.997500</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1.138141</td>\n",
       "      <td>35.764716</td>\n",
       "      <td>35.639493</td>\n",
       "      <td>34.641506</td>\n",
       "      <td>34.575435</td>\n",
       "      <td>35.783421</td>\n",
       "      <td>35.697344</td>\n",
       "      <td>35.732854</td>\n",
       "      <td>35.820831</td>\n",
       "      <td>35.792977</td>\n",
       "      <td>...</td>\n",
       "      <td>34.70051</td>\n",
       "      <td>34.690765</td>\n",
       "      <td>35.490428</td>\n",
       "      <td>35.2869</td>\n",
       "      <td>36.091883</td>\n",
       "      <td>35.781776</td>\n",
       "      <td>35.807031</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.867500</td>\n",
       "      <td>35.707500</td>\n",
       "      <td>35.582500</td>\n",
       "      <td>34.887500</td>\n",
       "      <td>35.087500</td>\n",
       "      <td>35.702500</td>\n",
       "      <td>35.680000</td>\n",
       "      <td>35.687500</td>\n",
       "      <td>35.725000</td>\n",
       "      <td>35.597500</td>\n",
       "      <td>...</td>\n",
       "      <td>34.21500</td>\n",
       "      <td>34.710000</td>\n",
       "      <td>35.152500</td>\n",
       "      <td>35.1175</td>\n",
       "      <td>35.972500</td>\n",
       "      <td>35.890000</td>\n",
       "      <td>35.917500</td>\n",
       "      <td>23.8</td>\n",
       "      <td>45.6</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     T_offset_mean  Max1R13_mean  Max1L13_mean  aveAllR13_mean  \\\n",
       "0         1.067500     35.600000     35.332500       35.305000   \n",
       "1         0.555000     36.112500     36.237500       35.535000   \n",
       "2         0.767500     37.620000     37.130000       37.162500   \n",
       "3         0.850000     35.490000     35.627500       34.865000   \n",
       "4         0.877500     34.735000     34.660000       34.102500   \n",
       "..             ...           ...           ...             ...   \n",
       "305       1.222500     35.642500     35.652500       34.857500   \n",
       "306       1.467500     35.982500     35.757500       35.427500   \n",
       "307       0.130000     36.407500     36.340000       35.870000   \n",
       "308       1.138141     35.764716     35.639493       34.641506   \n",
       "309       0.867500     35.707500     35.582500       34.887500   \n",
       "\n",
       "     aveAllL13_mean  T_RC_mean  T_RC_Dry_mean  T_RC_Wet_mean  T_RC_Max_mean  \\\n",
       "0         35.002500  35.585000      35.585000      35.395000      35.600000   \n",
       "1         35.810000  36.187500      36.187500      35.637500      36.200000   \n",
       "2         36.530000  37.695000      37.567500      37.690000      37.740000   \n",
       "3         34.962500  35.490000      35.482500      35.300000      35.517500   \n",
       "4         34.185000  34.747500      34.747500      34.377500      34.790000   \n",
       "..              ...        ...            ...            ...            ...   \n",
       "305       35.005000  35.752500      35.657500      35.737500      35.775000   \n",
       "306       35.197500  35.970000      35.950000      35.862500      36.007500   \n",
       "307       35.960000  36.410000      36.362500      36.365000      36.447500   \n",
       "308       34.575435  35.783421      35.697344      35.732854      35.820831   \n",
       "309       35.087500  35.702500      35.680000      35.687500      35.725000   \n",
       "\n",
       "     T_LC_mean  ...  T_FHBC_mean  T_FHTC_mean  T_FH_Max_mean  T_FHC_Max_mean  \\\n",
       "0    35.390000  ...     34.79250    34.837500      35.582500         35.3775   \n",
       "1    36.205000  ...     35.42250    35.307500      36.187500         35.6525   \n",
       "2    37.140000  ...     36.53000    35.420000      37.510000         36.9975   \n",
       "3    35.622500  ...     34.17750    33.882500      34.830000         34.5625   \n",
       "4    34.672500  ...     33.55750    33.200000      34.565000         34.5650   \n",
       "..         ...  ...          ...          ...            ...             ...   \n",
       "305  35.802500  ...     35.13750    35.275000      35.852500         35.7475   \n",
       "306  35.825000  ...     35.20750    35.070000      35.765000         35.5525   \n",
       "307  36.302500  ...     35.36750    35.342500      36.375000         35.7100   \n",
       "308  35.792977  ...     34.70051    34.690765      35.490428         35.2869   \n",
       "309  35.597500  ...     34.21500    34.710000      35.152500         35.1175   \n",
       "\n",
       "     T_Max_mean  T_OR_mean  T_OR_Max_mean  T_atm  Humidity  Distance  \n",
       "0     36.072500  35.970000      36.017500   22.0      30.0      0.60  \n",
       "1     36.720000  36.677500      36.720000   24.1      15.6      0.62  \n",
       "2     37.975000  37.595000      37.645000   24.1      15.6      0.62  \n",
       "3     36.107500  35.897500      35.970000   24.1      15.6      0.66  \n",
       "4     35.552500  35.395000      35.427500   24.1      18.0      0.60  \n",
       "..          ...        ...            ...    ...       ...       ...  \n",
       "305   36.067500  35.677500      35.710000   25.7      50.8      0.60  \n",
       "306   36.500000  36.452500      36.490000   25.7      50.8      0.60  \n",
       "307   36.535000  35.965000      35.997500   28.0      24.3      0.60  \n",
       "308   36.091883  35.781776      35.807031   25.0      39.8      0.60  \n",
       "309   35.972500  35.890000      35.917500   23.8      45.6      0.60  \n",
       "\n",
       "[310 rows x 30 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaning_columns = ['T_atm', 'Humidity', 'Distance']    \n",
    "remaning_features = X_test[remaning_columns].fillna(X_test[remaning_columns].mean())\n",
    "X_test = pd.concat([new_mean_dataframe, remaning_features], axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['aveAllL13_mean', 'T_offset_mean', 'T_FH_Max_mean', 'T_Max_mean', 'T_OR_Max_mean', 'T_RC_mean', 'T_atm', 'Max1R13_mean', 'Max1L13_mean', 'T_FHLC_mean', 'T_LC_Dry_mean', 'canthi4Max_mean', 'T_FHC_Max_mean', 'Distance', 'T_FHBC_mean']\n",
    "X_test_selected_features = X_test[columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aveAllL13_mean</th>\n",
       "      <th>T_offset_mean</th>\n",
       "      <th>T_FH_Max_mean</th>\n",
       "      <th>T_Max_mean</th>\n",
       "      <th>T_OR_Max_mean</th>\n",
       "      <th>T_RC_mean</th>\n",
       "      <th>T_atm</th>\n",
       "      <th>Max1R13_mean</th>\n",
       "      <th>Max1L13_mean</th>\n",
       "      <th>T_FHLC_mean</th>\n",
       "      <th>T_LC_Dry_mean</th>\n",
       "      <th>canthi4Max_mean</th>\n",
       "      <th>T_FHC_Max_mean</th>\n",
       "      <th>Distance</th>\n",
       "      <th>T_FHBC_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.095968</td>\n",
       "      <td>1.022227</td>\n",
       "      <td>35.542366</td>\n",
       "      <td>36.154939</td>\n",
       "      <td>35.896333</td>\n",
       "      <td>35.761603</td>\n",
       "      <td>24.574194</td>\n",
       "      <td>35.681540</td>\n",
       "      <td>35.715820</td>\n",
       "      <td>34.709807</td>\n",
       "      <td>35.714964</td>\n",
       "      <td>35.863587</td>\n",
       "      <td>35.233094</td>\n",
       "      <td>0.628214</td>\n",
       "      <td>34.621615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.641707</td>\n",
       "      <td>0.401755</td>\n",
       "      <td>0.518969</td>\n",
       "      <td>0.515670</td>\n",
       "      <td>0.596062</td>\n",
       "      <td>0.596181</td>\n",
       "      <td>1.159642</td>\n",
       "      <td>0.618556</td>\n",
       "      <td>0.584568</td>\n",
       "      <td>0.671719</td>\n",
       "      <td>0.575979</td>\n",
       "      <td>0.563694</td>\n",
       "      <td>0.548472</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.662395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>33.397500</td>\n",
       "      <td>-0.325000</td>\n",
       "      <td>34.465000</td>\n",
       "      <td>35.175000</td>\n",
       "      <td>34.097500</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>33.897500</td>\n",
       "      <td>34.122500</td>\n",
       "      <td>32.122500</td>\n",
       "      <td>34.105000</td>\n",
       "      <td>34.495000</td>\n",
       "      <td>33.737500</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>32.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34.740625</td>\n",
       "      <td>0.823499</td>\n",
       "      <td>35.245625</td>\n",
       "      <td>35.823750</td>\n",
       "      <td>35.521250</td>\n",
       "      <td>35.377977</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>35.273750</td>\n",
       "      <td>35.338125</td>\n",
       "      <td>34.328668</td>\n",
       "      <td>35.338125</td>\n",
       "      <td>35.486250</td>\n",
       "      <td>34.863750</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>34.274375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.053750</td>\n",
       "      <td>1.000748</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>36.070000</td>\n",
       "      <td>35.873750</td>\n",
       "      <td>35.688750</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>35.612500</td>\n",
       "      <td>35.632500</td>\n",
       "      <td>34.738750</td>\n",
       "      <td>35.623750</td>\n",
       "      <td>35.757357</td>\n",
       "      <td>35.201250</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>34.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.436875</td>\n",
       "      <td>1.212286</td>\n",
       "      <td>35.800000</td>\n",
       "      <td>36.326250</td>\n",
       "      <td>36.164375</td>\n",
       "      <td>36.055000</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>35.981250</td>\n",
       "      <td>35.990000</td>\n",
       "      <td>35.083750</td>\n",
       "      <td>35.976250</td>\n",
       "      <td>36.113302</td>\n",
       "      <td>35.514375</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>34.961250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>37.680000</td>\n",
       "      <td>2.555000</td>\n",
       "      <td>38.002500</td>\n",
       "      <td>38.417500</td>\n",
       "      <td>37.902500</td>\n",
       "      <td>38.385000</td>\n",
       "      <td>29.100000</td>\n",
       "      <td>38.405000</td>\n",
       "      <td>38.042500</td>\n",
       "      <td>37.165000</td>\n",
       "      <td>38.037500</td>\n",
       "      <td>38.382500</td>\n",
       "      <td>37.632500</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>37.212500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       aveAllL13_mean  T_offset_mean  T_FH_Max_mean  T_Max_mean  \\\n",
       "count      310.000000     310.000000     310.000000  310.000000   \n",
       "mean        35.095968       1.022227      35.542366   36.154939   \n",
       "std          0.641707       0.401755       0.518969    0.515670   \n",
       "min         33.397500      -0.325000      34.465000   35.175000   \n",
       "25%         34.740625       0.823499      35.245625   35.823750   \n",
       "50%         35.053750       1.000748      35.500000   36.070000   \n",
       "75%         35.436875       1.212286      35.800000   36.326250   \n",
       "max         37.680000       2.555000      38.002500   38.417500   \n",
       "\n",
       "       T_OR_Max_mean   T_RC_mean       T_atm  Max1R13_mean  Max1L13_mean  \\\n",
       "count     310.000000  310.000000  310.000000    310.000000    310.000000   \n",
       "mean       35.896333   35.761603   24.574194     35.681540     35.715820   \n",
       "std         0.596062    0.596181    1.159642      0.618556      0.584568   \n",
       "min        34.097500   34.000000   22.000000     33.897500     34.122500   \n",
       "25%        35.521250   35.377977   24.000000     35.273750     35.338125   \n",
       "50%        35.873750   35.688750   24.200000     35.612500     35.632500   \n",
       "75%        36.164375   36.055000   24.900000     35.981250     35.990000   \n",
       "max        37.902500   38.385000   29.100000     38.405000     38.042500   \n",
       "\n",
       "       T_FHLC_mean  T_LC_Dry_mean  canthi4Max_mean  T_FHC_Max_mean  \\\n",
       "count   310.000000     310.000000       310.000000      310.000000   \n",
       "mean     34.709807      35.714964        35.863587       35.233094   \n",
       "std       0.671719       0.575979         0.563694        0.548472   \n",
       "min      32.122500      34.105000        34.495000       33.737500   \n",
       "25%      34.328668      35.338125        35.486250       34.863750   \n",
       "50%      34.738750      35.623750        35.757357       35.201250   \n",
       "75%      35.083750      35.976250        36.113302       35.514375   \n",
       "max      37.165000      38.037500        38.382500       37.632500   \n",
       "\n",
       "         Distance  T_FHBC_mean  \n",
       "count  310.000000   310.000000  \n",
       "mean     0.628214    34.621615  \n",
       "std      0.043500     0.662395  \n",
       "min      0.540000    32.840000  \n",
       "25%      0.600000    34.274375  \n",
       "50%      0.600000    34.625000  \n",
       "75%      0.660000    34.961250  \n",
       "max      0.720000    37.212500  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_selected_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_test_original = scaler.fit_transform(X_test_selected_features)\n",
    "X_test_original = pd.DataFrame(X_test_original, columns=X_test_selected_features.columns)\n",
    "\n",
    "pca = PCA(n_components = 5)\n",
    "X_test_original = pca.fit_transform(X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.057948344736137185\n",
      "0.24072462428288716\n",
      "0.18316093939952338\n",
      "0.09822285795662443\n",
      "0.31340526153308984\n",
      "0.23424274713212573\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(X_train_original, y_train_original, X_test_original, y_test_original, kernel='linear', C=10, gamma='auto'):\n",
    "    \n",
    "    if kernel == 'linear':\n",
    "            clf = svm.SVR(kernel=kernel, C=C)\n",
    "    elif kernel == 'rbf':\n",
    "            clf = svm.SVR(kernel=kernel, C=C, gamma=gamma)\n",
    "    else:\n",
    "            raise ValueError(\"Invalid kernel type.\")\n",
    "\n",
    "    clf.fit(X_train_original, y_train_original)\n",
    "\n",
    "    # Make predictions on the training set\n",
    "    train_predictions = clf.predict(X_train_original)\n",
    "        \n",
    "    train_mse = mean_squared_error(y_train_original, train_predictions)\n",
    "    train_rmse = np.sqrt(train_mse)\n",
    "    train_mae = mean_absolute_error(y_train_original, train_predictions)\n",
    "    \n",
    "    test_predictions = clf.predict(X_test_original)\n",
    "\n",
    "    test_mse = mean_squared_error(y_test_original, test_predictions)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(y_test_original, test_predictions)\n",
    "        \n",
    "    print(train_mse)       \n",
    "    print(train_rmse)   \n",
    "    print(train_mae)\n",
    "    \n",
    "    print(test_mse)       \n",
    "    print(test_rmse)   \n",
    "    print(test_mae) \n",
    "\n",
    "train_and_evaluate(X_train_original, y_train_original, X_test_original, y_test_original, kernel='rbf')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee541_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
